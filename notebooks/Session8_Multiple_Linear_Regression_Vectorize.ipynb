{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21b694d0",
   "metadata": {
    "id": "21b694d0"
   },
   "source": [
    "# Exercise 5. Vectorize Linear Regression with Multiple Variables\n",
    "\n",
    "In this exercise, you will have to vectorized the  methods that appears with the heading **Vectorized**. Vectorization using NumPy allows for significantly faster computations compared to explicit loops, especially when dealing with large datasets. This is because NumPy operations are optimized and can leverage underlying C implementations for efficiency.\n",
    "\n",
    "Specifically you have to vectorize the following methods:\n",
    "\n",
    "* **Section 4** --> def compute_cost(X, y, w, b):\n",
    "\n",
    "* **Section 5** --> def compute_gradient(X, y, w, b):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g5E9AKld56TK",
   "metadata": {
    "id": "g5E9AKld56TK"
   },
   "source": [
    "# 1 Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c557274",
   "metadata": {
    "id": "1c557274"
   },
   "source": [
    "## 1.1 Goals\n",
    "- Extend our regression model  routines to support multiple features\n",
    "    - Extend data structures to support multiple features\n",
    "    - Rewrite prediction, cost and gradient routines to support multiple features\n",
    "    - Utilize NumPy `np.dot` to vectorize their implementations for speed and simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abb68a5",
   "metadata": {
    "id": "4abb68a5"
   },
   "source": [
    "<a name=\"toc_15456_1.2\"></a>\n",
    "## 1.2 Tools\n",
    "In this exercise, we will make use of:\n",
    "- NumPy, a popular library for scientific computing\n",
    "- Matplotlib, a popular library for plotting data"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8048c0c",
   "metadata": {
    "id": "c8048c0c"
   },
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "wFrh7MjBGnf5",
   "metadata": {
    "id": "wFrh7MjBGnf5"
   },
   "source": [
    "def load_house_data():\n",
    "    data = np.loadtxt(\n",
    "        \"https://drive.google.com/uc?id=1AkapW4GntdONcHy7God30DETeUPfgax6\",\n",
    "        delimiter=\",\",\n",
    "        skiprows=1,\n",
    "    )\n",
    "\n",
    "    X = data[:, :4]\n",
    "    y = data[:, 4]\n",
    "\n",
    "    # Fix sample with zero bedrooms\n",
    "    X[X[:, 1] == 0, 1] = 1\n",
    "\n",
    "    return X, y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fnGDATMt5-st",
   "metadata": {
    "id": "fnGDATMt5-st"
   },
   "source": [
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "def feedback(excercise_id, finished=True, understood=True, comment=\"No comments\"):\n",
    "    base_url = \"https://docs.google.com/forms/d/e/\"\n",
    "    base_url += \"1FAIpQLSf_oFpEq-gFAZMBwqTpipz6fbCRX1tqKjxMw_zXoNgEcBhjrg\"\n",
    "    base_url += \"/formResponse?\"\n",
    "    base_url += \"entry.2058183318=\" + excercise_id\n",
    "    base_url += \"&entry.217828241=\" + (\"Yes\" if finished else \"No\")\n",
    "    base_url += \"&entry.676697552=\" + (\"Yes\" if understood else \"No\")\n",
    "    base_url += \"&entry.379660172=\" + comment\n",
    "\n",
    "    base_url = base_url.replace(\" \", \"%20\")\n",
    "    # urlopen(base_url)\n",
    "    print(\"FEEDBACK SUBMITTED. THANKS!!! :D\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1t2niNa0PSOs",
   "metadata": {
    "id": "1t2niNa0PSOs"
   },
   "source": [
    "$\\mathbf{a}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0b2789",
   "metadata": {
    "id": "ed0b2789"
   },
   "source": [
    "## 1.3 Notation\n",
    "Here is a summary of some of the notation you will encounter, updated for multiple features.\n",
    "\n",
    "*   **General Notation**\n",
    "    *   $a$: scalar, non bold\n",
    "    *   $\\mathbf{a}$: vector, bold\n",
    "    *   $\\mathbf{A}$: matrix, bold capital\n",
    "\n",
    "*   **Regression Parameters**\n",
    "    *   $\\mathbf{X}$: training example matrix (`X_train`)\n",
    "    *   $\\mathbf{y}$: training example targets (`y_train`)\n",
    "    *   $\\mathbf{x}^{(i)}$, $y^{(i)}$: $i_{th}$ Training Example (`X[i]`, `y[i]`)\n",
    "    *   m: number of training examples (`m`)\n",
    "    *   n: number of features in each example (`n`)\n",
    "    *   $\\mathbf{w}$: parameter: weight vector (`w`)\n",
    "    *   $b$: parameter: bias (`b`)\n",
    "    *   $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$: The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$ (`f_wb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M9FvD2hWKJA7",
   "metadata": {
    "id": "M9FvD2hWKJA7"
   },
   "source": [
    "# 2 Problem Statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd4360f",
   "metadata": {
    "id": "dfd4360f"
   },
   "source": [
    "## 2.1 Data Set\n",
    "We will use the motivating example of housing price prediction. The training data set contains many examples with 4 features (size, bedrooms, floors and age) shown in the table below. Note, in this exercise, the Size feature is in sqft while earlier exercise utilized 1000 sqft.\n",
    "\n",
    "We would like to build a linear regression model using these values so we can then predict the price for other houses - say, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old."
   ]
  },
  {
   "cell_type": "code",
   "id": "07651b00",
   "metadata": {
    "id": "07651b00"
   },
   "source": [
    "# load the dataset\n",
    "X_train, y_train = load_house_data()\n",
    "X_features = [\"size\", \"bedrooms\", \"floors\", \"age\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "UvodVVEbHer6",
   "metadata": {
    "id": "UvodVVEbHer6"
   },
   "source": [
    "Let's view the dataset and its features by plotting each feature versus price."
   ]
  },
  {
   "cell_type": "code",
   "id": "sWajHUVYHd7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "sWajHUVYHd7c",
    "outputId": "490c8e29-3015-4525-b300-d8f59b77e976"
   },
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:, i], y_train)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"Price (1000's)\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a1defc20",
   "metadata": {
    "id": "a1defc20"
   },
   "source": [
    "## 2.2 Matrix X containing our examples\n",
    "Similar to the table above, examples are stored in a NumPy matrix `X_train`. Each row of the matrix represents one example. When you have $m$ training examples ( $m$ is three in our example), and there are $n$ features (four in our example), $\\mathbf{X}$ is a matrix with dimensions ($m$, $n$) (m rows, n columns).\n",
    "\n",
    "\n",
    "$$\\mathbf{X} =\n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\\n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "notation:\n",
    "- $\\mathbf{x}^{(i)}$ is vector containing example i. $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
    "- $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element.\n",
    "\n",
    "Display the input data."
   ]
  },
  {
   "cell_type": "code",
   "id": "a070dcc9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a070dcc9",
    "outputId": "1c0011d5-8f7b-47d6-9894-d8a8bb572076"
   },
   "source": [
    "# data is stored in numpy array/matrix\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ef9d6451",
   "metadata": {
    "id": "ef9d6451"
   },
   "source": [
    "<a name=\"toc_15456_2.2\"></a>\n",
    "## 2.3 Parameter vector w, b\n",
    "\n",
    "* $\\mathbf{w}$ is a vector with $n$ elements.\n",
    "  - Each element contains the parameter associated with one feature.\n",
    "  - in our dataset, n is 4.\n",
    "  - notionally, we draw this as a column vector\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "* $b$ is a scalar parameter."
   ]
  },
  {
   "cell_type": "code",
   "id": "13886e28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13886e28",
    "outputId": "f3a8dd3e-86d0-4b34-b890-69d2c814c24e"
   },
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cdbfcf47",
   "metadata": {
    "id": "cdbfcf47"
   },
   "source": [
    "<a name=\"toc_15456_3\"></a>\n",
    "# 3 Model Prediction With Multiple Variables\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$\n",
    "where $\\cdot$ is a vector `dot product`\n",
    "\n",
    "To demonstrate the dot product, we will implement prediction using (1) and (2)."
   ]
  },
  {
   "cell_type": "code",
   "id": "37f40302",
   "metadata": {
    "id": "37f40302"
   },
   "source": [
    "def predict(x, w, b):\n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (m,n) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters\n",
    "      b (scalar):             model parameter\n",
    "\n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b\n",
    "    return p"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe42e2c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe42e2c2",
    "outputId": "3b3ef361-4ed1-4eb4-f6e3-f9770ef2213f"
   },
   "source": [
    "# Make predictions for the whole training data\n",
    "f_wb = predict(X_train, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction for first row: {f_wb[0]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7098e6f2",
   "metadata": {
    "id": "7098e6f2"
   },
   "source": [
    "<a name=\"toc_15456_4\"></a>\n",
    "# 4 [VECTORIZE] Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$\n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$\n",
    "\n",
    "\n",
    "In contrast to tje previous exercise, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
   ]
  },
  {
   "cell_type": "code",
   "id": "c54d46f5",
   "metadata": {
    "id": "c54d46f5"
   },
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters\n",
    "      b (scalar)       : model parameter\n",
    "\n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "\n",
    "    f_wb = np.dot(X, w) + b\n",
    "\n",
    "    cost = np.sum((f_wb - y) ** 2) / (2 * m)\n",
    "\n",
    "    return cost"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce0b205d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce0b205d",
    "outputId": "94d68ace-78dd-48c4-b45e-f1741e2ca1b0"
   },
   "source": [
    "# Compute and display cost using our pre-chosen optimal parameters.\n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f\"Cost at initial w,b: {cost}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3451bfe0",
   "metadata": {
    "id": "3451bfe0"
   },
   "source": [
    "<a name=\"toc_15456_5\"></a>\n",
    "# 5 [VECTORIZE] Gradient Descent With Multiple Variables\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8d86a339",
   "metadata": {
    "id": "8d86a339"
   },
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters\n",
    "      b (scalar)       : model parameter\n",
    "\n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "\n",
    "    error = predict(X, w, b) - y\n",
    "\n",
    "    dj_dw = np.dot(X.T, error) / m\n",
    "    dj_db = np.sum(error) / m\n",
    "\n",
    "    return dj_db, dj_dw"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2f7fc87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2f7fc87",
    "outputId": "764ba435-c418-40ed-c0d0-6b761aff5342"
   },
   "source": [
    "# Compute and display gradient\n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f\"dj_db at initial w,b: {tmp_dj_db}\")\n",
    "print(f\"dj_dw at initial w,b: \\n {tmp_dj_dw}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d6e1f110",
   "metadata": {
    "id": "d6e1f110"
   },
   "source": [
    "def gradient_descent(\n",
    "    X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
    "    num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters\n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "\n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters\n",
    "      b (scalar)       : Updated value of parameter\n",
    "    \"\"\"\n",
    "\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  # avoid modifying global w within function\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w, b)  ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw  ##None\n",
    "        b = b - alpha * dj_db  ##None\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000:  # prevent resource exhaustion\n",
    "            J_history.append(cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "\n",
    "    return w, b, J_history  # return final w,b and J history for graphing"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8QVQ2km0TEFE",
   "metadata": {
    "id": "8QVQ2km0TEFE"
   },
   "source": [
    "# 6 Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0fa5f9",
   "metadata": {
    "id": "2a0fa5f9"
   },
   "source": [
    "## 6.1 Runing Gradient Descent without Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "id": "5a0454a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a0454a6",
    "outputId": "5a47fea3-1b9e-4525-c3eb-6ae06661981f"
   },
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.0000001\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist = gradient_descent(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    initial_w,\n",
    "    initial_b,\n",
    "    compute_cost,\n",
    "    compute_gradient,\n",
    "    alpha,\n",
    "    iterations,\n",
    ")\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m, _ = X_train.shape\n",
    "for i in range(3):\n",
    "    print(\n",
    "        f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6c45028",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "e6c45028",
    "outputId": "6b6dc900-4021-43a6-bf62-370689019312"
   },
   "source": [
    "# plot cost versus iteration\n",
    "plt.plot(J_hist)\n",
    "plt.title(\"Cost vs. iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"iteration step\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6I0BZ6ElTHgI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "6I0BZ6ElTHgI",
    "outputId": "bc7328c4-b96d-40e5-b2a4-e49b205f9bbc"
   },
   "source": [
    "# predict target using normalized features\n",
    "m = X_train.shape[0]\n",
    "yp = np.zeros(m)\n",
    "for i in range(m):\n",
    "    yp[i] = np.dot(X_train[i], w_final) + b_final\n",
    "\n",
    "    # plot predictions and targets versus original features\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:, i], y_train, label=\"target\")\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "    ax[i].scatter(X_train[:, i], yp, color=\"orange\", label=\"predict\")\n",
    "ax[0].set_ylabel(\"Price\")\n",
    "ax[0].legend()\n",
    "fig.suptitle(\"target versus prediction\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "WX__dxS4XZSv",
   "metadata": {
    "id": "WX__dxS4XZSv"
   },
   "source": [
    "\n",
    "## 6.2 Z-score normalization\n",
    "\n",
    "After z-score normalization, all features will have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "To implement z-score normalization, adjust your input values as shown in this formula:\n",
    "$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\tag{4}$$\n",
    "where $j$ selects a feature or a column in the $\\mathbf{X}$ matrix. $µ_j$ is the mean of all the values for feature (j) and $\\sigma_j$ is the standard deviation of feature (j).\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ">**Implementation Note:** When normalizing the features, it is important\n",
    "to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters\n",
    "from the model, we often want to predict the prices of houses we have not\n",
    "seen before. Given a new x value (living room area and number of bed-\n",
    "rooms), we must first normalize x using the mean and standard deviation\n",
    "that we had previously computed from the training set.\n",
    "\n",
    "**Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "id": "iQyhqEK9Whks",
   "metadata": {
    "id": "iQyhqEK9Whks"
   },
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    computes  X, zcore normalized by column\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n))     : input data, m examples, n features\n",
    "\n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "      mu (ndarray (n,))     : mean of each feature\n",
    "      sigma (ndarray (n,))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # find the mean of each column/feature\n",
    "    X_mean = np.mean(X, axis=0)  # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    X_sigma = np.std(X, axis=0)  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - X_mean) / X_sigma\n",
    "\n",
    "    return (X_norm, X_mean, X_sigma)\n",
    "\n",
    "\n",
    "# check our work\n",
    "# from sklearn.preprocessing import scale\n",
    "# scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e2qdtWDfYsNs",
   "metadata": {
    "id": "e2qdtWDfYsNs"
   },
   "source": [
    "Apply data normalization"
   ]
  },
  {
   "cell_type": "code",
   "id": "aRCVopZsXxt1",
   "metadata": {
    "id": "aRCVopZsXxt1"
   },
   "source": [
    "X_norm, X_mean, X_sigma = zscore_normalize_features(X_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "kuYOhkqlYl47",
   "metadata": {
    "id": "kuYOhkqlYl47"
   },
   "source": [
    "Plotting the data after normalization (compare with the output of Section 2.1)"
   ]
  },
  {
   "cell_type": "code",
   "id": "jyavf4yvXjtC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "jyavf4yvXjtC",
    "outputId": "5b569a6d-6dc2-4bfd-b2d9-fd7bd469c017"
   },
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_norm[:, i], y_train)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "\n",
    "ax[0].set_ylabel(\"Price (1000's)\")\n",
    "fig.suptitle(\"Data After Z-Score Normalization\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "HxOp0dUEX2t7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "HxOp0dUEX2t7",
    "outputId": "e085f745-ae0e-44ba-c1c4-ce26303dba18"
   },
   "source": [
    "mu = np.mean(X_train, axis=0)\n",
    "sigma = np.std(X_train, axis=0)\n",
    "X_mean = X_train - mu\n",
    "X_norm = (X_train - mu) / sigma\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "ax[0].scatter(X_train[:, 0], X_train[:, 3])\n",
    "ax[0].set_xlabel(X_features[0])\n",
    "ax[0].set_ylabel(X_features[3])\n",
    "ax[0].set_title(\"unnormalized\")\n",
    "ax[0].axis(\"equal\")\n",
    "\n",
    "ax[1].scatter(X_mean[:, 0], X_mean[:, 3])\n",
    "ax[1].set_xlabel(X_features[0])\n",
    "ax[0].set_ylabel(X_features[3])\n",
    "ax[1].set_title(r\"X - $\\mu$\")\n",
    "ax[1].axis(\"equal\")\n",
    "\n",
    "ax[2].scatter(X_norm[:, 0], X_norm[:, 3])\n",
    "ax[2].set_xlabel(X_features[0])\n",
    "ax[0].set_ylabel(X_features[3])\n",
    "ax[2].set_title(r\"Z-score normalized\")\n",
    "ax[2].axis(\"equal\")\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.suptitle(\"distribution of features before, during, after normalization\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "pBqizM3rZNA-",
   "metadata": {
    "id": "pBqizM3rZNA-"
   },
   "source": [
    "## 6.2 Runing Gradient Descent after Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "id": "YeubKqQbZNA-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YeubKqQbZNA-",
    "outputId": "ae5c27da-d9a2-49f1-a4c3-2ffdaf291abb"
   },
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.0\n",
    "# some gradient descent settings\n",
    "iterations = 20000\n",
    "alpha = 0.001\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist = gradient_descent(\n",
    "    X_norm,\n",
    "    y_train,\n",
    "    initial_w,\n",
    "    initial_b,\n",
    "    compute_cost,\n",
    "    compute_gradient,\n",
    "    alpha,\n",
    "    iterations,\n",
    ")\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m, _ = X_train.shape\n",
    "for i in range(3):\n",
    "    print(\n",
    "        f\"prediction: {np.dot(X_norm[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "o63-0S86ZNA-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "o63-0S86ZNA-",
    "outputId": "88134368-e302-4fd4-9f1a-e284029845b5"
   },
   "source": [
    "# plot cost versus iteration\n",
    "plt.plot(J_hist[100:])\n",
    "plt.title(\"Cost vs. iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"iteration step\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "yAzMDpB2ZNA_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "yAzMDpB2ZNA_",
    "outputId": "54c863c7-c92c-42eb-f9f0-3320ddcd28d5"
   },
   "source": [
    "# predict target using normalized features\n",
    "m = X_norm.shape[0]\n",
    "yp = np.zeros(m)\n",
    "for i in range(m):\n",
    "    yp[i] = np.dot(X_norm[i], w_final) + b_final\n",
    "\n",
    "    # plot predictions and targets versus original features\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:, i], y_train, label=\"target\")\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "    ax[i].scatter(X_train[:, i], yp, color=\"orange\", label=\"predict\")\n",
    "ax[0].set_ylabel(\"Price\")\n",
    "ax[0].legend()\n",
    "fig.suptitle(\"target versus prediction\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8mEcVvQJ5MjI",
   "metadata": {
    "id": "8mEcVvQJ5MjI"
   },
   "source": [
    "## Exercise 1: Explore the effect of feature scaling in Gradient Descent\n",
    "\n",
    "Perform the following experiments:\n",
    "\n",
    "* Observe how the learning rate has different effects before and after normalization.\n",
    "\n",
    "* Observe how quickly gradient descent converges depending of the learning rate before and after normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oe71c7JzMIzb",
   "metadata": {
    "id": "oe71c7JzMIzb"
   },
   "source": [
    "\n",
    "---\n",
    "SUBMIT FEEDBACK\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "l2wSP8DiMIzc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2wSP8DiMIzc",
    "outputId": "a042a40c-dc4f-47d6-92cb-d82821e1d2ee"
   },
   "source": [
    "feedback(\n",
    "    \"session3_MLR_Exercise1\", finished=True, understood=False, comment=\"No comments\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ZM3SAhhUnoPx",
   "metadata": {
    "id": "ZM3SAhhUnoPx"
   },
   "source": [
    "# 7 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rTOJZFcl7BC_",
   "metadata": {
    "id": "rTOJZFcl7BC_"
   },
   "source": [
    "Let us introduce in our training data sample a new feature \"size_per_bedrrom\":\n",
    "\n",
    "$$ \\text{size_per_bedroom} = \\frac{\\text{size}}{\\text{number of bedrooms}}$$"
   ]
  },
  {
   "cell_type": "code",
   "id": "p0LyehrRkKjc",
   "metadata": {
    "id": "p0LyehrRkKjc"
   },
   "source": [
    "# Create a larger matrix\n",
    "X_train_extended = np.zeros((X_train.shape[0], X_train.shape[1] + 1))\n",
    "\n",
    "# Copy the training data to the new matrix\n",
    "X_train_extended[:, :-1] = X_train\n",
    "\n",
    "# Add another new colum\n",
    "X_train_extended[:, -1] = X_train[:, 0] / (X_train[:, 1])\n",
    "\n",
    "# Create new names for the variable\n",
    "X_features_extended = [\"size\", \"bedrooms\", \"floors\", \"age\", \"size_per_bedroom\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "XiSecQaH7nI8",
   "metadata": {
    "id": "XiSecQaH7nI8"
   },
   "source": [
    "Let us look how the new data set looks like:"
   ]
  },
  {
   "cell_type": "code",
   "id": "iTCHoKXdlNHg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "iTCHoKXdlNHg",
    "outputId": "15ba4461-be7d-40cc-a0c3-50a7dfcd9244"
   },
   "source": [
    "# predict target using normalized features\n",
    "m = X_train_extended.shape[0]\n",
    "\n",
    "# plot predictions and targets versus original features\n",
    "fig, ax = plt.subplots(1, X_train_extended.shape[1], figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train_extended[:, i], y_train, label=\"target\")\n",
    "    ax[i].set_xlabel(X_features_extended[i])\n",
    "ax[0].set_ylabel(\"Price\")\n",
    "ax[0].legend()\n",
    "fig.suptitle(\"Extended Data\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9RaNo21P7tNC",
   "metadata": {
    "id": "9RaNo21P7tNC"
   },
   "source": [
    "Let us normalize the new data set"
   ]
  },
  {
   "cell_type": "code",
   "id": "MtEe4NEUmPKa",
   "metadata": {
    "id": "MtEe4NEUmPKa"
   },
   "source": [
    "X_norm, X_mean, X_sigma = zscore_normalize_features(X_train_extended)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "lYykBUd67vnU",
   "metadata": {
    "id": "lYykBUd67vnU"
   },
   "source": [
    "Let us learn with the new data set"
   ]
  },
  {
   "cell_type": "code",
   "id": "oIAhbsoeliMu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIAhbsoeliMu",
    "outputId": "33862fad-0b46-4063-a3f4-1a4f6eed67ba"
   },
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros(X_norm.shape[1])\n",
    "initial_b = 0.0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.01\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist = gradient_descent(\n",
    "    X_norm,\n",
    "    y_train,\n",
    "    initial_w,\n",
    "    initial_b,\n",
    "    compute_cost,\n",
    "    compute_gradient,\n",
    "    alpha,\n",
    "    iterations,\n",
    ")\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m, _ = X_train.shape\n",
    "for i in range(3):\n",
    "    print(\n",
    "        f\"prediction: {np.dot(X_norm[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "OoHi3ZrR7yUI",
   "metadata": {
    "id": "OoHi3ZrR7yUI"
   },
   "source": [
    "Let us inspect the learning results of the new data set"
   ]
  },
  {
   "cell_type": "code",
   "id": "YNje_4SUmXtC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "YNje_4SUmXtC",
    "outputId": "ddab8dfd-2dbc-4833-cc69-b5cd4292763b"
   },
   "source": [
    "# predict target using normalized features\n",
    "m = X_norm.shape[0]\n",
    "yp = np.zeros(m)\n",
    "for i in range(m):\n",
    "    yp[i] = np.dot(X_norm[i], w_final) + b_final\n",
    "\n",
    "    # plot predictions and targets versus original features\n",
    "fig, ax = plt.subplots(1, X_norm.shape[1], figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train_extended[:, i], y_train, label=\"target\")\n",
    "    ax[i].set_xlabel(X_features_extended[i])\n",
    "    ax[i].scatter(X_train_extended[:, i], yp, color=\"orange\", label=\"predict\")\n",
    "ax[0].set_ylabel(\"Price\")\n",
    "ax[0].legend()\n",
    "fig.suptitle(\"target versus prediction\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "uTcBbDFfoGfO",
   "metadata": {
    "id": "uTcBbDFfoGfO"
   },
   "source": [
    "## Exercise 2: Add a new feature and see what happens\n",
    "\n",
    "* Add a new feature you think it could help to better predict the price of a house, following the same approach used to add \"size_per_bedroom\" feature. (i) Justify why did you decided to use this feature? (ii) Explore the training cost of the new solution an compare it with the cost of the previous solutions. Is the cost smaller?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebUixEv7sQ0",
   "metadata": {
    "id": "aebUixEv7sQ0"
   },
   "source": [
    "\n",
    "---\n",
    "SUBMIT FEEDBACK\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "86pC0ToK7sQ-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86pC0ToK7sQ-",
    "outputId": "8031c17d-17e8-4af2-c620-703933931b57"
   },
   "source": [
    "feedback(\n",
    "    \"session3_MLR_Exercise2\", finished=True, understood=True, comment=\"No comments\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "tozv-bdlAjWb",
   "metadata": {
    "id": "tozv-bdlAjWb"
   },
   "source": [
    "# 8 Overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rf_ZdPBpMrRY",
   "metadata": {
    "id": "rf_ZdPBpMrRY"
   },
   "source": [
    "## 8.1 Adding Polynomial Features\n",
    "\n",
    "Let us add new polynomial features (i.e. $size^2$, $size^3$, etc) to our training data set and see what happens with the cost funciton after running gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "id": "erz5jqCsAlJd",
   "metadata": {
    "id": "erz5jqCsAlJd"
   },
   "source": [
    "# Enter the degree of the new polynomial features.\n",
    "degree = 20\n",
    "# Create a larger matrix\n",
    "X_train, y_train = load_house_data()\n",
    "X_train, _, _ = zscore_normalize_features(X_train)\n",
    "X_train_extended = np.zeros((X_train.shape[0], X_train.shape[1] + degree - 1))\n",
    "\n",
    "# Copy the training data to the new matrix\n",
    "if degree > 1:\n",
    "    X_train_extended[:, : -(degree - 1)] = X_train\n",
    "else:\n",
    "    X_train_extended = X_train\n",
    "\n",
    "X_features_extended = [\"size\", \"bedrooms\", \"floors\", \"age\"]\n",
    "\n",
    "for d in range(2, degree + 1):\n",
    "    # Add the new colum for \"size^d\"\n",
    "    X_train_extended[:, X_train.shape[1] + d - 2] = X_train[:, 0] ** d\n",
    "    # Create new names for the variable\n",
    "    X_features_extended.append(\"size^\" + str(d))\n",
    "\n",
    "X_train_extended, _, _ = zscore_normalize_features(X_train_extended)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3Q-5swJDWqt5",
   "metadata": {
    "id": "3Q-5swJDWqt5"
   },
   "source": [
    "##  Exercise 3: Polynomial Features\n",
    "\n",
    "- Add polynomial features using the code above. Start with degree=2 and move to degree=20.\n",
    "\n",
    "- Look how the Cost function is reduced.\n",
    "\n",
    "- Do you think we are doing better by having more polynomial functions?"
   ]
  },
  {
   "cell_type": "code",
   "id": "q9Ah0-enWt3s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9Ah0-enWt3s",
    "outputId": "065ea43f-f6ee-4906-a767-64cdee4c0dbd"
   },
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros(X_train_extended.shape[1])\n",
    "initial_b = 0.0\n",
    "# some gradient descent settings\n",
    "iterations = 100000\n",
    "alpha = 0.1\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist = gradient_descent(\n",
    "    X_train_extended,\n",
    "    y_train,\n",
    "    initial_w,\n",
    "    initial_b,\n",
    "    compute_cost,\n",
    "    compute_gradient,\n",
    "    alpha,\n",
    "    iterations,\n",
    ")\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m, _ = X_train.shape\n",
    "for i in range(3):\n",
    "    print(\n",
    "        f\"prediction: {np.dot(X_train_extended[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "uNMXkPnw7u5R",
   "metadata": {
    "id": "uNMXkPnw7u5R"
   },
   "source": [
    "\n",
    "---\n",
    "SUBMIT FEEDBACK\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "eFzVspsT7u5S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFzVspsT7u5S",
    "outputId": "a1a75c83-3d62-43fb-8aee-4a5e3b30ef70"
   },
   "source": [
    "feedback(\n",
    "    \"session3_MLR_Exercise3\", finished=True, understood=True, comment=\"No comments\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6FHtQSNvD906",
   "metadata": {
    "id": "6FHtQSNvD906"
   },
   "source": [
    "## 8.2 Measuring Overfitting\n",
    "\n",
    "We now split the data set in training data set and validation data set."
   ]
  },
  {
   "cell_type": "code",
   "id": "L8nXv9ojDmKp",
   "metadata": {
    "id": "L8nXv9ojDmKp"
   },
   "source": [
    "# Permutate the data\n",
    "np.random.seed(123)\n",
    "perm = np.random.permutation(X_train_extended.shape[0])\n",
    "X_train_extended = X_train_extended[perm, :]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "# Split the data\n",
    "size_train = 66\n",
    "X_split_train, y_split_train = X_train_extended[0:size_train, :], y_train[0:size_train]\n",
    "X_split_val, y_split_val = X_train_extended[size_train:, :], y_train[size_train:]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0HoPdlZ0EvD9",
   "metadata": {
    "id": "0HoPdlZ0EvD9"
   },
   "source": [
    "Let us plot both data sets"
   ]
  },
  {
   "cell_type": "code",
   "id": "Tqyvvfp5EanZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "Tqyvvfp5EanZ",
    "outputId": "73372920-8824-42f6-e18a-1f6e4647a468"
   },
   "source": [
    "# plot the training data set\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_split_train[:, i], y_split_train, label=\"target\")\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"Price\")\n",
    "ax[0].legend()\n",
    "fig.suptitle(\"Training Data Set\")\n",
    "plt.show()\n",
    "\n",
    "# plot predictions and targets versus original features\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_split_val[:, i], y_split_val, label=\"target\")\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"Price\")\n",
    "ax[0].legend()\n",
    "fig.suptitle(\"Validation Data Set\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5fpyO22l73AX",
   "metadata": {
    "id": "5fpyO22l73AX"
   },
   "source": [
    "The function below implements gradient descent, but it computes both the cost for the training data set and for the validation data set"
   ]
  },
  {
   "cell_type": "code",
   "id": "cQ_pUMTPE3zU",
   "metadata": {
    "id": "cQ_pUMTPE3zU"
   },
   "source": [
    "def gradient_descent_val(\n",
    "    X, y, X_val, y_val, w_in, b_in, cost_function, gradient_function, alpha, num_iters\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
    "    num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters\n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "\n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters\n",
    "      b (scalar)       : Updated value of parameter\n",
    "    \"\"\"\n",
    "\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history_train = []\n",
    "    J_history_test = []\n",
    "    w = copy.deepcopy(w_in)  # avoid modifying global w within function\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w, b)  ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw  ##None\n",
    "        b = b - alpha * dj_db  ##None\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000:  # prevent resource exhaustion\n",
    "            J_history_train.append(cost_function(X, y, w, b))\n",
    "            J_history_test.append(cost_function(X_val, y_val, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(\n",
    "                f\"Iteration {i:4d}: Training Cost {J_history_train[-1]:8.2f}, Test cost {J_history_test[-1]:8.2f}   \"\n",
    "            )\n",
    "\n",
    "    return w, b, J_history_train, J_history_test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "lGt4kKh-8Eju",
   "metadata": {
    "id": "lGt4kKh-8Eju"
   },
   "source": [
    "##  Exercise 4: Measuing Training and Test Error\n",
    "\n",
    "- Add polynomial features using the code above. Start with degree=2 and move to degree=20.\n",
    "\n",
    "- Look how the Cost function for the training and the test set evolves.\n",
    "\n",
    "- Increase and decrease the number of iterations for gradient descent (move it from 10000 to 20000) and observed what happens with the the training and the test cost.\n",
    "\n",
    "- What do you think is happening?\n",
    "\n",
    "- Which should be the degree of the included polynomial features?"
   ]
  },
  {
   "cell_type": "code",
   "id": "LKR33nRyGvde",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKR33nRyGvde",
    "outputId": "9afc4847-5347-4cb7-9188-0a4d24df607f"
   },
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros(X_split_train.shape[1])\n",
    "initial_b = 0.0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.01\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist_train, J_history_test = gradient_descent_val(\n",
    "    X_split_train,\n",
    "    y_split_train,\n",
    "    X_split_val,\n",
    "    y_split_val,\n",
    "    initial_w,\n",
    "    initial_b,\n",
    "    compute_cost,\n",
    "    compute_gradient,\n",
    "    alpha,\n",
    "    iterations,\n",
    ")\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m, _ = X_train.shape\n",
    "for i in range(3):\n",
    "    print(\n",
    "        f\"prediction: {np.dot(X_split_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "KoxdMJGvUqII",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "KoxdMJGvUqII",
    "outputId": "059fbc5f-62db-49ff-880f-c956a9172d7d"
   },
   "source": [
    "# plot cost versus iteration\n",
    "plt.plot(np.arange(500, iterations, 1), J_hist_train[500:], label=\"Training Cost\")\n",
    "plt.plot(np.arange(500, iterations, 1), J_history_test[500:], label=\"Test Cost\")\n",
    "plt.title(\"Training/Test Cost vs. iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"iteration step\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0luum9S79Fcj",
   "metadata": {
    "id": "0luum9S79Fcj"
   },
   "source": [
    "\n",
    "---\n",
    "SUBMIT FEEDBACK\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "JK-aNtm69Fco",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JK-aNtm69Fco",
    "outputId": "b5a06c70-bf79-4c11-c523-9ad23101e038"
   },
   "source": [
    "feedback(\n",
    "    \"session3_MLR_Exercise4\", finished=True, understood=True, comment=\"No comments\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "dl_toc_settings": {
   "rndtag": "15456"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
