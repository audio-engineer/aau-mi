{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfcbed6e",
   "metadata": {
    "id": "cfcbed6e"
   },
   "source": [
    "# <div align=\"center\" style=\"font-size: 50px;\">Machine Intelligence Exam Part II</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f1e370",
   "metadata": {
    "id": "73f1e370"
   },
   "source": [
    "### **Instructions**:\n",
    "\n",
    "- Students are allowed to use any notes and material they want (physical or electronic).\n",
    "- Students are not allowed to use any online resource.\n",
    "- Students are not allowed to use any generative AI tool or any automatic code generation.\n",
    "- Students are allowed to use any IDE they want (e.g., Visual Code, PyCharm).\n",
    "- The IDE must not have any generative AI tool activated for code generation.\n",
    "- Recommended python version is Python 3.\n",
    "- You need to install, at least, the following packages: ``numpy``, ``matplotlib``, ``pandas``, ``jupyter`` , ``tensorflow``and ``keras``.\n",
    "\n",
    "\n",
    "### **Submission**:\n",
    "\n",
    "- The submission must be a single Python notebook.\n",
    "- The notebook must be named *MI_Exam.ipynb*.\n",
    "- **The notebook must be submitted in Digital Exam before the end of the exam**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g5E9AKld56TK",
   "metadata": {
    "id": "g5E9AKld56TK"
   },
   "source": [
    "# Imports and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f9d26",
   "metadata": {
    "id": "f72f9d26"
   },
   "source": [
    "This code imports essential Python libraries for data manipulation, numerical computing, and visualization, and sets NumPy array print precision to two decimal places for cleaner output."
   ]
  },
  {
   "cell_type": "code",
   "id": "c8048c0c",
   "metadata": {
    "id": "c8048c0c"
   },
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1dcf56cb",
   "metadata": {
    "id": "1dcf56cb"
   },
   "source": [
    "\n",
    "This code defines a function `load_data()` that loads a bike sharing dataset from a CSV file, processes it by removing certain columns, and returns the feature matrix X, target values y, feature names, and index values in a format suitable for machine learning tasks. If loading from the Google Drive URL fails, the function will automatically attempt to load the data from a local file \"./Train.csv\" which should be placed in the same directory as the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "-----------------------------------------------------\n",
    "\n",
    "Look at the code of the method `load_data()` to know in which folder you have to place the given file `Data.csv` containing the dataset. Alternatively, modify the code of the method to properly read the file `Data.csv`.\n",
    "\n",
    "-----------------------------------------------------\n"
   ],
   "metadata": {
    "id": "3RcpH0ffqbHH"
   },
   "id": "3RcpH0ffqbHH"
  },
  {
   "cell_type": "code",
   "id": "wFrh7MjBGnf5",
   "metadata": {
    "id": "wFrh7MjBGnf5"
   },
   "source": [
    "def load_data():\n",
    "    try:\n",
    "        # First try loading from URL\n",
    "        df = pd.read_csv(\"https://drive.google.com/uc?id=1AeJXQa_BpRrUwUbzmQlqzFhMafOnuWwx\", sep=',')\n",
    "    except:\n",
    "        # If URL fails, try loading from local file\n",
    "        print(\"URL load failed. Attempting to load from local file...\")\n",
    "        try:\n",
    "            df = pd.read_csv(\"./Data.csv\", sep=',')\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"Could not load data from URL or local file './Train.csv'\")\n",
    "\n",
    "    # Rest of the function remains unchanged\n",
    "    features = list(df)\n",
    "    del features[0]\n",
    "    del features[0]\n",
    "    del features[len(features)-1]\n",
    "\n",
    "    data = df.to_numpy()\n",
    "    data = np.delete(data,1,axis=1)\n",
    "    data = data.astype('float64')\n",
    "\n",
    "    index = data[:,0]\n",
    "    X = data[:,1:-1]\n",
    "    y = data[:,-1]\n",
    "\n",
    "    return X, y, features, index"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "M9FvD2hWKJA7",
   "metadata": {
    "id": "M9FvD2hWKJA7"
   },
   "source": [
    "# The Predictive Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e1f18",
   "metadata": {
    "id": "b22e1f18"
   },
   "source": [
    "\n",
    "\n",
    "### Description\n",
    "\n",
    "Bike-sharing systems provide a convenient means of renting bicycles, with automated processes for membership, rental, and returns facilitated through a network of kiosks distributed across a city. These systems allow users to rent a bike from one location and return it to another, as needed. Globally, there are over 500 bike-sharing programs.\n",
    "\n",
    "The data collected by these systems is highly valuable for researchers. Information such as travel duration, departure and arrival locations, and elapsed time is explicitly recorded, making these systems function as a dynamic sensor network for studying urban mobility. In this competition, participants are tasked with leveraging historical usage patterns and weather data to predict bike rental demand for the Capital Bikeshare program in Washington, D.C.\n",
    "\n",
    "---\n",
    "\n",
    "### Predictive Task\n",
    "\n",
    "You are provided with hourly bike rental data spanning two years. The training set includes data from the first 19 days of each month, while the test set covers data from the 20th day to the end of the month. Your objective is to predict whether bike rental demand during each hour in the test set was **high** or **low**, using only information available prior to the rental period.\n",
    "\n",
    "---\n",
    "\n",
    "### Data Fields\n",
    "\n",
    "- **yr**: Year (0 for 2011, 1 for 2012).\n",
    "- **month**: Month of the year (1-12).\n",
    "- **day**: Day of the month (1-31).\n",
    "- **hour**: Hour of the day (0-23).\n",
    "- **season**: Season of the year:\n",
    "  - 1 = Spring\n",
    "  - 2 = Summer\n",
    "  - 3 = Fall\n",
    "  - 4 = Winter\n",
    "- **holiday**: Indicator of whether the day is a holiday (1 = holiday, 0 = non-holiday).\n",
    "- **weekday**: Day of the week (0-6)\n",
    "- **workingday**: Indicator of whether the day is a working day (1 = working day, 0 = weekend or holiday).\n",
    "- **weather**: Weather conditions categorized as:\n",
    "  - 1: Clear, Few clouds, Partly cloudy\n",
    "  - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds\n",
    "  - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds\n",
    "  - 4: Heavy Rain + Ice Pellets + Thunderstorm + Mist, Snow + Fog\n",
    "- **temp**: Actual temperature (normalized value).\n",
    "- **atemp**: Perceived \"feels-like\" temperature (normalized value).\n",
    "- **humidity**: Relative humidity percentage ((normalized value).\n",
    "- **windspeed**: Wind speed (normalized value).\n",
    "- **demand**: Target variable indicating rental demand:\n",
    "  - 0 (or False): Low demand\n",
    "  - 1 (or True): High demand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd4360f",
   "metadata": {
    "id": "dfd4360f"
   },
   "source": [
    "# 1. Data Exploration and Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a59364",
   "metadata": {
    "id": "07a59364"
   },
   "source": [
    "This code loads the bike sharing training dataset into features (X_train), target values (y_train), feature names (X_features), and index values (index) using the load_data() function."
   ]
  },
  {
   "cell_type": "code",
   "id": "07651b00",
   "metadata": {
    "id": "07651b00",
    "outputId": "4299e05f-cd1b-41c4-e30e-4cd32e67e59f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# load the dataset\n",
    "X_train, y_train, X_features, index = load_data()\n",
    "\n",
    "# data is stored in numpy array/matrix\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "- The following visualization is organized into **12 subplots** arranged in a **3x4 grid**, with each subplot representing the relationship between a specific feature and bike rental demand.\n",
    "\n",
    "- Each bar in the subplots illustrates the **percentage distribution** between:\n",
    "  - **High demand** (green)\n",
    "  - **Low demand** (red)\n",
    "\n",
    "- For **numeric features** (e.g., temperature, feels-like temperature, humidity, and windspeed):\n",
    "  - The data is **discretized into 4 bins** to improve clarity and make the visualization more interpretable.\n",
    "\n",
    "- This approach allows for an easy comparison of how different feature values influence rental demand.\n"
   ],
   "metadata": {
    "id": "i73YeYXTIlSe"
   },
   "id": "i73YeYXTIlSe"
  },
  {
   "cell_type": "code",
   "source": [
    "# Create subplots for each feature\n",
    "fig, ax = plt.subplots(3, 4, figsize=(20, 15))\n",
    "fig.suptitle('High vs Low Demand Distribution by Features', fontsize=20)\n",
    "\n",
    "# Flatten ax array for easier iteration\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(len(ax)):\n",
    "    feature_vals = X_train[:, i]\n",
    "\n",
    "    # For numeric features (temp, atemp, humidity, windspeed), create bins\n",
    "    if X_features[i] in ['temp', 'atemp', 'humidity', 'windspeed']:\n",
    "        # Create 4 bins\n",
    "        bins = np.linspace(feature_vals.min(), feature_vals.max(), 5)\n",
    "        labels = [f'{bins[j]:.1f}-{bins[j+1]:.1f}' for j in range(len(bins)-1)]\n",
    "        feature_vals = np.digitize(feature_vals, bins[1:-1])\n",
    "        unique_vals = np.arange(len(labels))\n",
    "        x_labels = labels\n",
    "    else:\n",
    "        # For categorical features, use as-is\n",
    "        unique_vals = np.unique(feature_vals)\n",
    "        x_labels = [str(int(val)) for val in unique_vals]\n",
    "\n",
    "    high_demand = []\n",
    "    low_demand = []\n",
    "\n",
    "    # Calculate counts for high and low demand\n",
    "    for val in unique_vals:\n",
    "        mask = feature_vals == val\n",
    "        total = np.sum(mask)\n",
    "        if total > 0:\n",
    "            high_count = np.sum(y_train[mask] == 1)\n",
    "            low_count = np.sum(y_train[mask] == 0)\n",
    "            # Convert to percentages\n",
    "            high_demand.append((high_count / total) * 100)\n",
    "            low_demand.append((low_count / total) * 100)\n",
    "\n",
    "    # Create stacked bar plot\n",
    "    ax[i].bar(x_labels, low_demand, label='Low Demand', color='red', alpha=0.7)\n",
    "    ax[i].bar(x_labels, high_demand, bottom=low_demand, label='High Demand', color='green', alpha=0.7)\n",
    "\n",
    "    ax[i].set_xlabel(X_features[i], fontsize=14)\n",
    "    ax[i].set_ylabel('Percentage (%)', fontsize=14)\n",
    "    ax[i].tick_params(axis='x', rotation=45 if len(x_labels) > 4 else 0)\n",
    "    ax[i].grid(True, linestyle='--', alpha=0.3)\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax[i].legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ErN-EB-4ImI0",
    "outputId": "82fe7fab-8943-4dbb-d7df-906b5f20fed1"
   },
   "id": "ErN-EB-4ImI0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b16db08e",
   "metadata": {
    "id": "b16db08e"
   },
   "source": [
    "## TASK 1: Data Analysis (15 min)\n",
    "\n",
    "Looking at the figures, choose what you think is the most and least informative feature to predict bike demand. Justify your answer.\n",
    "\n",
    "Introduce your answer in the cell below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be022c",
   "metadata": {
    "id": "f5be022c"
   },
   "source": [
    "### Answer (approx 200 words)\n",
    "[Introduce your answer here]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbfcf47",
   "metadata": {
    "id": "cdbfcf47"
   },
   "source": [
    "# 2. Model Defintion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90126d1d",
   "metadata": {
    "id": "90126d1d"
   },
   "source": [
    "The provided code snippet demonstrates how to build, train, and evaluate a **logistic regression model** using Keras, a popular deep learning library. The model is designed to predict high demand in a bike-sharing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd1e51",
   "metadata": {
    "id": "06bd1e51"
   },
   "source": [
    "\n",
    "\n",
    "## TASK 2A: HYPERPARAMETER TUNING (30 min)\n",
    "\n",
    "\n",
    "When training your logistic regression model using Keras, selecting the right hyperparameters is essential to ensure optimal performance and generalization to unseen data. In this task, you are required to determine suitable values for three key hyperparameters: the **learning rate**, the **number of epochs**, and the **validation split**. Each of these decisions plays a critical role in shaping the training process and the model's final accuracy.\n",
    "\n",
    "For each hyperparameter, provide a clear rationale for your chosen values. To guide your reasoning, consider the following:\n",
    "\n",
    "- **Learning Rate**:\n",
    "  How does your selected learning rate balance the trade-off between model convergence speed and training stability? What potential issues, such as slow convergence or overshooting, could arise with suboptimal values?\n",
    "\n",
    "- **Number of Epochs**:\n",
    "  Why is your chosen number of epochs appropriate, given the complexity of your dataset and the risk of overfitting or underfitting? How do you plan to monitor performance during training to refine this choice?\n",
    "\n",
    "- **Validation Split**:\n",
    "  How does your chosen validation split ensure a robust and reliable evaluation of your model’s performance during training? What considerations (e.g., dataset size or class balance) influenced this decision?\n",
    "\n",
    "Provide your reasoning and selected values in the cell below. Be sure to connect your choices to the specifics of your dataset and model to demonstrate a thoughtful approach to hyperparameter tuning.\n",
    "\n",
    "**If the training takes longer than 2 min consider stop the training and do not explore these configurations.**\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "95c48c93",
   "metadata": {
    "id": "95c48c93",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 549
    },
    "outputId": "9335b706-e7cb-48dc-e734-968d998549ce"
   },
   "source": [
    "# Import necessary libraries from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))  # Logistic regression\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model and store history\n",
    "history = model.fit(X_train, y_train,\n",
    "                   epochs=1,\n",
    "                   batch_size=32,\n",
    "                   validation_split=0.01,\n",
    "                   verbose=1)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b8a19db8",
   "metadata": {
    "id": "b8a19db8"
   },
   "source": [
    "### Answer (approx 300 words)\n",
    "[Introduce your answer here]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TASK 2B: Regularization (15 min)\n",
    "\n",
    "Regularization is a key technique in machine learning that prevents overfitting by adding a penalty to the loss function. Here you will explore how regularization affects the performance of a logistic regression model on the bike-sharing dataset.\n",
    "\n",
    "1. Train a logistic regression models using Keras and L2 regularization.\n",
    "* Use the Dense layer in Keras with the `kernel_regularizer` parameter to apply the regularization.\n",
    "* Experiment with different values of the regularization strength and note their impact.\n",
    "\n",
    "2. Evaluate the Models:\n",
    "\n",
    "* Compare the performance of both models using accuracy and validation loss."
   ],
   "metadata": {
    "id": "NtcPi5FQXEbe"
   },
   "id": "NtcPi5FQXEbe"
  },
  {
   "cell_type": "code",
   "source": [
    "# Import necessary libraries from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=X_train.shape[1], kernel_regularizer=regularizers.L2(0.0), activation='sigmoid'))  # Logistic regression\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model and store history\n",
    "history = model.fit(X_train, y_train,\n",
    "                   epochs=1,\n",
    "                   batch_size=32,\n",
    "                   validation_split=0.01,\n",
    "                   verbose=1)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ],
   "metadata": {
    "id": "G4E35rqEl85g",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "outputId": "63084dbf-8212-4b61-94c0-2ab22d7525c7"
   },
   "id": "G4E35rqEl85g",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answer\n",
    " 1. Briefly explain in the purpose of regularization and how L2 regularization works. (150 words)\n",
    "\n",
    "**[Introduce your answer here]**\n",
    "\n",
    "\n",
    " 2. Discuss how the regularization terms affected the models in terms of overfitting and underfitting. (200 words)\n",
    "\n",
    "**[Introduce your answer here]**\n"
   ],
   "metadata": {
    "id": "caVKNzhOXuOK"
   },
   "id": "caVKNzhOXuOK"
  },
  {
   "cell_type": "markdown",
   "id": "f48602b4",
   "metadata": {
    "id": "f48602b4"
   },
   "source": [
    "## TASK 2C: CREATING A MULTI-LAYER PERCEPTRON (30 min)\n",
    "\n",
    "Now that you have gained experience training models, your next task is to create a **multilayer perceptron (MLP)** using Keras and train it as you did in previous exercises. Follow the steps below to guide your experimentation:\n",
    "\n",
    "- **Create and Experiment with Different Architectures**:\n",
    "  - Vary the **number of layers**.\n",
    "  - Adjust the **number of neurons in each layer (width)** .\n",
    "  - Use **different activation functions**.\n",
    "\n",
    "-  **Train the MLP with L2 regularization**.\n",
    "   - Use the Dense layer in Keras with the kernel_regularizer parameter to apply the regularization.\n",
    "   - Experiment with different values of the regularization strength and note their impact.\n",
    "\n",
    "- **Train and Evaluate**:\n",
    "  - Train each configuration and monitor the key metrics (e.g., accuracy, loss) to compare different configurations.\n",
    "\n",
    "- **Analyze and Discuss**:\n",
    "  - Discuss your findings and justify which one would be the best configuration for this dataset and problem.\n",
    "\n",
    "\n",
    "**If the training takes longer than 2 min consider stop the training and do not explore these configurations.**\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "dad1b046",
   "metadata": {
    "id": "dad1b046",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "outputId": "d296d5dc-4c50-4ff2-a176-a51ad46987b2"
   },
   "source": [
    "#Introduce the code for the MLP here\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model and store history\n",
    "history = model.fit(X_train, y_train,\n",
    "                   epochs=1,\n",
    "                   batch_size=32,\n",
    "                   validation_split=0.01,\n",
    "                   verbose=1)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Print final metrics\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "33cb25b3",
   "metadata": {
    "id": "33cb25b3"
   },
   "source": [
    "### Answer (300 words)\n",
    "\n",
    "[Introduce your answer here]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c466ad68",
   "metadata": {
    "id": "c466ad68"
   },
   "source": [
    "\n",
    "# Task 3. Discussion of Key Take-aways (30 min)\n",
    "\n",
    "In this section, you are expected to articulate the most relevant insights gained throughout the workflow. Use precise terminology consistent with the course material and ensure that your discussion explicitly connects to concepts introduced in the lectures and exercises.\n",
    "\n",
    "## 3a. Dataset Take-aways (approx. 100 words)\n",
    "[Insert your reflection on data quality, feature distributions, preprocessing steps, potential biases, missingness patterns, and any domain-specific considerations that influenced modelling decisions.]\n",
    "\n",
    "## 3b. Model Take-aways (approx. 100 words)\n",
    "[Insert your discussion of model selection rationale, inductive biases, hyperparameter settings, training dynamics, strengths and limitations of the chosen architecture, and how these relate to theoretical concepts covered in the course.]\n",
    "\n",
    "## 3c. Evaluation Take-aways (approx. 100 words)\n",
    "[Insert your analysis of evaluation metrics, validation strategy, overfitting/underfitting diagnosis, error patterns, model robustness, and how the results inform the model’s expected generalization performance.]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  },
  "dl_toc_settings": {
   "rndtag": "15456"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-autonumbering": false,
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
