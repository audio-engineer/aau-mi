{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQENrtZYNfz3"
   },
   "source": [
    "# Session: Uncertainty Estimation in Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57uJ7ULdNfz6"
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vpAkLUKyNfz6"
   },
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U8fy8kzFNfz9",
    "outputId": "53a2ce85-44d4-4a1c-d334-45d2cc475af1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "print(\"Tensorflow Version: \", tf.__version__)\n",
    "print(\"Tensorflow Probability Version: \", tfp.__version__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYb1RxzPNfz-"
   },
   "source": [
    "![MNIST and MNIST-corrupted overview image](https://github.com/goodboychan/goodboychan.github.io/blob/main/_notebooks/image/mnist_corrupted.png?raw=1)\n",
    "\n",
    "# The MNIST and MNIST-C datasets\n",
    "\n",
    "In this notebook, you will use the [MNIST](http://yann.lecun.com/exdb/mnist/) and [MNIST-C](https://github.com/google-research/mnist-c) datasets, which both consist of a training set of 60,000 handwritten digits with corresponding labels, and a test set of 10,000 images. The images have been normalised and centred. The MNIST-C dataset is a corrupted version of the MNIST dataset, to test out-of-distribution robustness of computer vision models.\n",
    "\n",
    "- Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE, 86(11):2278-2324, November 1998.\n",
    "- N. Mu and J. Gilmeer. \"MNIST-C: A Robustness Benchmark for Computer Vision\" https://arxiv.org/abs/1906.02337\n",
    "\n",
    "Our goal is to construct a neural network that classifies images of handwritten digits into one of 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmd-HVxCNfz-"
   },
   "source": [
    "### Load the datasets\n",
    "\n",
    "We'll start by importing two datasets. The first is the MNIST dataset of handwritten digits, and the second is the MNIST-C dataset, which is a corrupted version of the MNIST dataset. This dataset is available on [TensorFlow datasets](https://www.tensorflow.org/datasets/catalog/mnist_corrupted). We'll be using the dataset with \"spatters\". We will load and inspect the datasets below. We'll use the notation `_c` to denote `corrupted`. The images are the same as in the original MNIST, but are \"corrupted\" by some grey spatters."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "!mkdir MNIST\n",
    "!mkdir MNIST_corrupted"
   ],
   "metadata": {
    "id": "s6WFeaI2cVT_",
    "cellView": "form"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "import requests\n",
    "\n",
    "r2 = requests.get(\n",
    "    \"https://github.com/goodboychan/goodboychan.github.io/raw/main/_notebooks/dataset/MNIST/x_train.npy\"\n",
    ")\n",
    "f = open(\"MNIST/x_train.npy\", \"wb\")\n",
    "f.write(r2.content)\n",
    "f.close()\n",
    "\n",
    "r2 = requests.get(\n",
    "    \"https://github.com/goodboychan/goodboychan.github.io/raw/main/_notebooks/dataset/MNIST/y_train.npy\"\n",
    ")\n",
    "f = open(\"MNIST/y_train.npy\", \"wb\")\n",
    "f.write(r2.content)\n",
    "f.close()\n",
    "\n",
    "r2 = requests.get(\n",
    "    \"https://github.com/goodboychan/goodboychan.github.io/raw/main/_notebooks/dataset/MNIST/x_test.npy\"\n",
    ")\n",
    "f = open(\"MNIST/x_test.npy\", \"wb\")\n",
    "f.write(r2.content)\n",
    "f.close()\n",
    "\n",
    "r2 = requests.get(\n",
    "    \"https://github.com/goodboychan/goodboychan.github.io/raw/main/_notebooks/dataset/MNIST/y_test.npy\"\n",
    ")\n",
    "f = open(\"MNIST/y_test.npy\", \"wb\")\n",
    "f.write(r2.content)\n",
    "f.close()"
   ],
   "metadata": {
    "id": "DzmtuUYuZNki",
    "cellView": "form"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "r2 = requests.get(\n",
    "    \"https://github.com/goodboychan/goodboychan.github.io/raw/main/_notebooks/dataset/MNIST_corrupted/x_train.npy\"\n",
    ")\n",
    "f = open(\"MNIST_corrupted/x_train.npy\", \"wb\")\n",
    "f.write(r2.content)\n",
    "f.close()\n",
    "\n",
    "r2 = requests.get(\n",
    "    \"https://github.com/goodboychan/goodboychan.github.io/raw/main/_notebooks/dataset/MNIST_corrupted/y_train.npy\"\n",
    ")\n",
    "f = open(\"MNIST_corrupted/y_train.npy\", \"wb\")\n",
    "f.write(r2.content)\n",
    "f.close()\n",
    "\n",
    "r2 = requests.get(\n",
    "    \"https://github.com/goodboychan/goodboychan.github.io/raw/main/_notebooks/dataset/MNIST_corrupted/x_test.npy\"\n",
    ")\n",
    "f = open(\"MNIST_corrupted/x_test.npy\", \"wb\")\n",
    "f.write(r2.content)\n",
    "f.close()\n",
    "\n",
    "r2 = requests.get(\n",
    "    \"https://github.com/goodboychan/goodboychan.github.io/raw/main/_notebooks/dataset/MNIST_corrupted/y_test.npy\"\n",
    ")\n",
    "f = open(\"MNIST_corrupted/y_test.npy\", \"wb\")\n",
    "f.write(r2.content)\n",
    "f.close()"
   ],
   "metadata": {
    "id": "_ynUdciTbugY",
    "cellView": "form"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "24t4FlBDNfz_",
    "cellView": "form"
   },
   "source": [
    "# @title\n",
    "# Function to load training and testing data, with labels in integer and one-hot form\n",
    "\n",
    "\n",
    "def load_data(name):\n",
    "    data_dir = name\n",
    "    x_train = 1 - np.load(os.path.join(data_dir, \"x_train.npy\")) / 255.0\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    y_train = np.load(os.path.join(data_dir, \"y_train.npy\"))\n",
    "    y_train_oh = tf.keras.utils.to_categorical(y_train)\n",
    "    x_test = 1 - np.load(os.path.join(data_dir, \"x_test.npy\")) / 255.0\n",
    "    x_test = x_test.astype(np.float32)\n",
    "    y_test = np.load(os.path.join(data_dir, \"y_test.npy\"))\n",
    "    y_test_oh = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "    return (x_train, y_train, y_train_oh), (x_test, y_test, y_test_oh)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BlmOk464Nf0B",
    "cellView": "form"
   },
   "source": [
    "# @title\n",
    "# Function to inspect dataset digits\n",
    "\n",
    "\n",
    "def inspect_images(data, num_images):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=num_images, figsize=(2 * num_images, 2))\n",
    "    for i in range(num_images):\n",
    "        ax[i].imshow(data[i, ..., 0], cmap=\"gray\")\n",
    "        ax[i].axis(\"off\")\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zQH2zIJwNf0C",
    "outputId": "8f89038d-6cab-479b-9565-c4be8799bca4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    }
   },
   "source": [
    "# Load and inspect the MNIST dataset\n",
    "(x_train, y_train, y_train_oh), (x_test, y_test, y_test_oh) = load_data(\"MNIST\")\n",
    "inspect_images(data=x_train, num_images=8)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CZWgOfFiNf0E",
    "outputId": "48220c8a-b023-40ce-c90b-1046ba077702",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    }
   },
   "source": [
    "# Load and inspect the MNIST-C dataset\n",
    "(x_c_train, y_c_train, y_c_train_oh), (x_c_test, y_c_test, y_c_test_oh) = load_data(\n",
    "    \"MNIST_corrupted\"\n",
    ")\n",
    "inspect_images(data=x_c_train, num_images=8)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOEyXcSINf0F"
   },
   "source": [
    "# Neural Networks as Probabilistic Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The **softmax activation function** is widely used in machine learning, especially in multi-class classification tasks. Its primary purpose is to convert a vector of raw scores (often called logits) into probabilities, where the sum of the probabilities is 1.\n",
    "\n",
    "### Definition\n",
    "\n",
    "For a vector $\\mathbf{z} = [z_1, z_2, \\ldots, z_K]$ of $K$ real-valued logits, the softmax function computes the probability of each class \\(i\\) as:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $e^{z_i}$ is the exponential of the $i$-th element of the vector.\n",
    "- $\\sum_{j=1}^K e^{z_j}$ is the sum of the exponentials of all elements in the vector.\n",
    "\n",
    "### Properties\n",
    "1. **Normalization**: The output is a probability distribution; all probabilities are non-negative, and their sum equals 1:\n",
    "   $$\n",
    "   \\sum_{i=1}^K \\text{softmax}(z_i) = 1.\n",
    "   $$\n",
    "\n",
    "2. **Amplification of Differences**: The exponential function amplifies larger logits, making higher scores more dominant in the resulting probability distribution. This property helps in focusing the model's confidence on a single class.\n",
    "\n",
    "3. **Continuous and Differentiable**: Softmax is smooth, which is essential for gradient-based optimization methods like stochastic gradient descent.\n",
    "\n",
    "### Intuition\n",
    "- If all elements of \\(\\mathbf{z}\\) are equal, the softmax function assigns equal probability to each class.\n",
    "- If one element of \\(\\mathbf{z}\\) is significantly larger than the others, the corresponding probability will be close to 1, while others will be near 0.\n",
    "\n",
    "### Applications\n",
    "1. **Output Layer for Classification**:\n",
    "   - In multi-class classification, the softmax function is used in the final layer of a neural network to model the probability distribution over \\(K\\) possible classes.\n",
    "   - The predicted class is the one with the highest softmax probability.\n",
    "\n",
    "2. **Loss Function**:\n",
    "   - It is often paired with the categorical cross-entropy loss to optimize classification tasks. Cross-entropy measures how close the predicted probability distribution is to the true distribution (one-hot encoded labels).\n",
    "\n",
    "### Example\n",
    "Suppose you have three logits: \\([2.0, 1.0, 0.1]\\).\n",
    "\n",
    "1. Compute exponentials:\n",
    "   $$\n",
    "   e^{2.0} \\approx 7.39, \\quad e^{1.0} \\approx 2.72, \\quad e^{0.1} \\approx 1.11\n",
    "   $$\n",
    "\n",
    "2. Compute the sum:\n",
    "   $$\n",
    "   7.39 + 2.72 + 1.11 = 11.22\n",
    "   $$\n",
    "\n",
    "3. Compute softmax probabilities:\n",
    "   $$\n",
    "   \\text{softmax}(2.0) = \\frac{7.39}{11.22} \\approx 0.658, \\quad\n",
    "   \\text{softmax}(1.0) = \\frac{2.72}{11.22} \\approx 0.242, \\quad\n",
    "   \\text{softmax}(0.1) = \\frac{1.11}{11.22} \\approx 0.099\n",
    "   $$\n",
    "\n",
    "### Insights\n",
    "- The class corresponding to \\(z = 2.0\\) has the highest probability (65.8%), indicating the model's confidence in that class.\n",
    "- Small changes in the logits can lead to non-linear changes in the softmax probabilities, making it sensitive to the relative differences between scores.\n"
   ],
   "metadata": {
    "id": "jW07TRzv8d16"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-AaW2lq7Nf0G",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a2021f3b-94ab-4cf6-9ef2-8be8497c6b2d"
   },
   "source": [
    "# Define the CNN model with softmax layer\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "cnn_model = Sequential(\n",
    "    [\n",
    "        Conv2D(\n",
    "            kernel_size=(5, 5),\n",
    "            filters=8,\n",
    "            activation=\"relu\",\n",
    "            padding=\"VALID\",\n",
    "            input_shape=(28, 28, 1),\n",
    "        ),\n",
    "        MaxPooling2D(pool_size=(6, 6)),\n",
    "        Flatten(),\n",
    "        # Adding SoftMax Layer makes the output a probability distribution\n",
    "        Dense(units=10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cnn_model.compile(\n",
    "    loss=SparseCategoricalCrossentropy(), optimizer=RMSprop(), metrics=[\"accuracy\"]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Zy5GE4OINf0H",
    "outputId": "e2554d45-0fbb-469e-e5d3-754ce0399c74",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    }
   },
   "source": [
    "# Print the model summary\n",
    "\n",
    "cnn_model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j3hn-_OfNf0I",
    "outputId": "97b4e310-92b6-485c-85ca-b7ea5742670a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Train the model\n",
    "\n",
    "cnn_model.fit(x_train, y_train, epochs=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wf6vaNMXNf0J",
    "outputId": "435c0de0-016e-45f7-f8c9-070d62371688",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Evaluate the model\n",
    "\n",
    "print(\n",
    "    \"Accuracy on MNIST test set: \",\n",
    "    str(cnn_model.evaluate(x_test, y_test, verbose=False)[1]),\n",
    ")\n",
    "print(\n",
    "    \"Accuracy on corrupted MNIST test set: \",\n",
    "    str(cnn_model.evaluate(x_c_test, y_c_test, verbose=False)[1]),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxmsu1vPNf0K"
   },
   "source": [
    "As you might expect, the pointwise performance on the corrupted MNIST set is worse. This makes sense, since this dataset is slightly different, and noisier, than the uncorrupted version. Furthermore, the model was trained on the uncorrupted MNIST data, so has no experience with the spatters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtxIdzLrNf0O"
   },
   "source": [
    "## Analyse the model predictions\n",
    "\n",
    "We will now do some deeper analysis by looking at the probabilities the model assigns to each class instead of its single prediction.\n",
    "\n",
    "The function below will be useful to help us analyse the probabilistic model predictions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nRsaS1chNf0O",
    "cellView": "form"
   },
   "source": [
    "# @title\n",
    "# Function to make plots of the probabilities that the model estimates for an image\n",
    "\n",
    "\n",
    "def analyse_model_prediction(data, true_labels, model, image_num, run_ensemble=False):\n",
    "    if run_ensemble:\n",
    "        ensemble_size = 200\n",
    "    else:\n",
    "        ensemble_size = 1\n",
    "    image = data[image_num]\n",
    "    true_label = true_labels[image_num, 0]\n",
    "    predicted_probabilities = np.empty(shape=(ensemble_size, 10))\n",
    "    for i in range(ensemble_size):\n",
    "        predicted_probabilities[i] = model(image[np.newaxis, :]).numpy()[0]\n",
    "    model_prediction = model(image[np.newaxis, :])\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        nrows=1, ncols=2, figsize=(10, 2), gridspec_kw={\"width_ratios\": [2, 4]}\n",
    "    )\n",
    "\n",
    "    # Show the image and the true label\n",
    "    ax1.imshow(image[..., 0], cmap=\"gray\")\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.set_title(\"True label: {}\".format(str(true_label)))\n",
    "\n",
    "    # Show a 95% prediction interval of model predicted probabilities\n",
    "    pct_2p5 = np.array(\n",
    "        [np.percentile(predicted_probabilities[:, i], 2.5) for i in range(10)]\n",
    "    )\n",
    "    pct_97p5 = np.array(\n",
    "        [np.percentile(predicted_probabilities[:, i], 97.5) for i in range(10)]\n",
    "    )\n",
    "    bar = ax2.bar(np.arange(10), pct_97p5, color=\"red\")\n",
    "    bar[int(true_label)].set_color(\"green\")\n",
    "    ax2.bar(\n",
    "        np.arange(10), pct_2p5 - 0.02, color=\"white\", linewidth=1, edgecolor=\"white\"\n",
    "    )\n",
    "    ax2.set_xticks(np.arange(10))\n",
    "    ax2.set_ylim([0, 1])\n",
    "    ax2.set_ylabel(\"Probability\")\n",
    "    ax2.set_title(\"Model estimated probabilities\")\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bsNZYu9kNf0R",
    "outputId": "19c2af86-c91b-4081-8c2c-e7889d5c3dd3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    }
   },
   "source": [
    "# Prediction examples on MNIST\n",
    "\n",
    "for i in [0, 1577]:\n",
    "    analyse_model_prediction(x_test, y_test, cnn_model, i)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4CyMctaNf0R"
   },
   "source": [
    "The model is very confident that the first image is a 6, which is correct. For the second image, the model struggles, assigning nonzero probabilities to many different classes.\n",
    "\n",
    "Run the code below to do the same for 2 images from the corrupted MNIST test set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fxt38p4wNf0U",
    "outputId": "3a932f8b-f9c8-4dac-8f3a-79b2c3734120",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    }
   },
   "source": [
    "# Prediction examples on MNIST-C\n",
    "\n",
    "for i in [0, 3702]:\n",
    "    analyse_model_prediction(x_c_test, y_c_test, cnn_model, i)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uaz1CY1wNf0U"
   },
   "source": [
    "The first is the same 6 as you saw above, but the second image is different. Notice how the model can still say with high certainty that the first image is a 6, but struggles for the second.\n",
    "\n",
    "Finally, have a look at an image for which the model is very sure on MNIST data but very unsure on corrupted MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1BqimVHqNf0V",
    "outputId": "022adaf8-2706-4d7a-bf42-9a57d9ce0aff",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    }
   },
   "source": [
    "# Prediction examples from both datasets\n",
    "\n",
    "for i in [9241]:\n",
    "    analyse_model_prediction(x_test, y_test, cnn_model, i)\n",
    "    analyse_model_prediction(x_c_test, y_c_test, cnn_model, i)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUdRzyYuNf0V"
   },
   "source": [
    "It's not surprising what's happening here: the spatters cover up most of the number. You would hope a model indicates that it's unsure here, since there's very little information to go by. This is exactly what's happened."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 1: Investigating Model Uncertainty in Digit Classification\n",
    "\n",
    "\n",
    "1. **Identify Uncertain Predictions**:\n",
    "   - Using a test dataset of digit images, identify cases where the model is **highly uncertain** about its prediction.\n",
    "   - Specifically, find examples where the top two predicted probabilities are close to each other (e.g., both >30% but <70%), indicating that the model is indecisive between two classes.\n",
    "\n",
    "2. **Explore Digit id \"988\"**:\n",
    "   - Examine the specific case of the digit image labeled with ID \"988\" in the dataset.\n",
    "   - Answer the following questions:\n",
    "     - What does the probability distribution output by the model look like for this digit?\n",
    "     - Do you think the uncertainty reported by the model reflects the inherent ambiguity of the image? Why or why not?\n",
    "   - Visualize the digit and provide a short explanation for your answer.\n",
    "   - Find Digits with similar issues.\n",
    "\n"
   ],
   "metadata": {
    "id": "ZJoIU0y78oBg"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChGga3I5Nf0V"
   },
   "source": [
    "## Uncertainty quantification using entropy\n",
    "\n",
    "We can also make some analysis of the model's uncertainty across the full test set, instead of for individual values. One way to do this is to calculate the [entropy](https://en.wikipedia.org/wiki/Entropy_%28information_theory%29) of the distribution. The entropy is the expected information (or informally, the expected 'surprise') of a random variable, and is a measure of the uncertainty of the random variable. The entropy of the estimated probabilities for sample $i$ is defined as\n",
    "\n",
    "$$\n",
    "H_i = -\\sum_{j=1}^{10} p_{ij} \\text{log}_{2}(p_{ij})\n",
    "$$\n",
    "\n",
    "where $p_{ij}$ is the probability that the model assigns to sample $i$ corresponding to label $j$. The entropy as above is measured in _bits_. If the natural logarithm is used instead, the entropy is measured in _nats_.\n",
    "\n",
    "The key point is that the higher the value, the more unsure the model is."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_entropy(x, labels, model, i):\n",
    "    probs = model(x).numpy()\n",
    "    entropy = -np.sum(probs * np.log2(probs), axis=1)\n",
    "    return entropy[i]"
   ],
   "metadata": {
    "id": "M5nWGx3d-3IO"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for i in [0, 3702]:\n",
    "    analyse_model_prediction(x_c_test, y_c_test, cnn_model, i)\n",
    "    print(f\"Entropy: {compute_entropy(x_c_test, y_c_test, cnn_model, i)}\")\n",
    "    print(f\"2\\^Entropy: {2**compute_entropy(x_c_test, y_c_test, cnn_model, i)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "7tCNt6kW-0pF",
    "outputId": "de5a511b-5af5-4da4-dd52-85762cb4156f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise2 : Exploring Entropy as a Measure of Prediction Uncertainty\n",
    "\n",
    "- Examine predictions for various digits and observe how the **entropy** reflects the level of uncertainty in the model's predictions. Use examples where the model is confident (low entropy) and uncertain (high entropy) to solidify your understanding.\n",
    "\n",
    "- Develop an intuition for how \\(2^{\\text{Entropy}}\\) approximates the **effective number of classes** over which the model's probabilities are distributed. Explore cases where the probabilities are concentrated on a few classes versus spread across many."
   ],
   "metadata": {
    "id": "oXFu5Nj1_21V"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Srvr-zQWAu1v"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Models are more unsure when they are wrong\n",
    "\n",
    "Let's see the distribution of the entropy of the model's predictions across the MNIST and corrupted MNIST test sets. The plots will be split between predictions the model gets correct and incorrect."
   ],
   "metadata": {
    "id": "l4xkhD0o_bGm"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F9aR0rpRNf0V"
   },
   "source": [
    "# @title\n",
    "# Functions to plot the distribution of the information entropy across samples,\n",
    "# split into whether the model prediction is correct or incorrect\n",
    "\n",
    "\n",
    "def get_correct_indices(model, x, labels):\n",
    "    y_model = model(x)\n",
    "    correct = np.argmax(y_model, axis=1) == np.squeeze(labels)\n",
    "    correct_indices = [i for i in range(x.shape[0]) if correct[i]]\n",
    "    incorrect_indices = [i for i in range(x.shape[0]) if not correct[i]]\n",
    "    return correct_indices, incorrect_indices\n",
    "\n",
    "\n",
    "def plot_entropy_distribution(model, x, labels):\n",
    "    probs = model(x).numpy()\n",
    "    entropy = -np.sum(probs * np.log2(probs), axis=1)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    for i, category in zip(range(2), [\"Correct\", \"Incorrect\"]):\n",
    "        entropy_category = entropy[get_correct_indices(model, x, labels)[i]]\n",
    "        mean_entropy = np.mean(entropy_category)\n",
    "        num_samples = entropy_category.shape[0]\n",
    "        title = category + \"ly labelled ({:.1f}% of total)\".format(\n",
    "            num_samples / x.shape[0] * 100\n",
    "        )\n",
    "        axes[i].hist(entropy_category, weights=(1 / num_samples) * np.ones(num_samples))\n",
    "        axes[i].annotate(\n",
    "            \"Mean: {:.3f} bits\".format(mean_entropy), (0.4, 0.9), ha=\"center\"\n",
    "        )\n",
    "        axes[i].set_xlabel(\"Entropy (bits)\")\n",
    "        axes[i].set_ylim([0, 1])\n",
    "        axes[i].set_ylabel(\"Probability\")\n",
    "        axes[i].set_title(title)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GWvNkEd7Nf0V",
    "outputId": "45530709-f5fc-489c-c9a6-32438af86772",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    }
   },
   "source": [
    "# Entropy plots for the MNIST dataset\n",
    "\n",
    "print(\"MNIST test set:\")\n",
    "plot_entropy_distribution(cnn_model, x_test, y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FjmwR7k2Nf0V",
    "outputId": "cc9e3528-749b-4998-f0a2-295b57114b42",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    }
   },
   "source": [
    "# Entropy plots for the MNIST-C dataset\n",
    "\n",
    "print(\"Corrupted MNIST test set:\")\n",
    "plot_entropy_distribution(cnn_model, x_c_test, y_c_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzCo6DZhNf0W"
   },
   "source": [
    "There are two main conclusions:\n",
    "- The model is more unsure on the predictions it got wrong: this means it \"knows\" when the prediction may be wrong.\n",
    "- The model is more unsure for the corrupted MNIST test than for the uncorrupted version. Futhermore, this is more pronounced for correct predictions than for those it labels incorrectly.\n",
    "\n",
    "In this way, the model seems to \"know\" when it is unsure. This is a great property to have in a machine learning model, and is one of the advantages of probabilistic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Using a Probabilistic Model for Selective Predictions\n",
    "\n",
    "When using a **probabilistic model** to make predictions, the model provides not only a predicted class but also a measure of its confidence (e.g., a probability or uncertainty score). This confidence can help decide when to make a prediction and when to refrain due to uncertainty.\n",
    "\n",
    "#### Selective Prediction\n",
    "In situations where the model's uncertainty is high, it might be better to avoid making a prediction rather than risking an incorrect one. This approach is particularly useful in critical applications, such as medical diagnosis or autonomous systems, where incorrect decisions can have serious consequences.\n",
    "\n",
    "The user can set a **confidence threshold**:\n",
    "- If the model's confidence for a prediction is above the threshold, the prediction is accepted.\n",
    "- If the confidence is below the threshold, the model \"abstains\" from making a prediction.\n",
    "\n",
    "This process ensures that predictions are made only when the model is sufficiently confident.\n",
    "\n",
    "#### Coverage\n",
    "\"Coverage\" refers to the **fraction of instances** for which the model makes predictions after applying the threshold. For example:\n",
    "- If the model is applied to 1,000 data points and makes predictions for 800 of them (abstaining on 200 due to low confidence), the coverage is:\n",
    "  \\[\n",
    "  \\text{Coverage} = \\frac{\\text{Number of predictions made}}{\\text{Total number of instances}} = \\frac{800}{1000} = 80\\%\n",
    "  \\]\n",
    "\n",
    "#### Accuracy Over Covered Instances\n",
    "After applying the threshold, you can compute the **accuracy** over the instances where the model did make predictions. This is often referred to as the **selective accuracy** or the accuracy over covered instances.\n",
    "\n",
    "Selective accuracy is a useful measure because it shows how well the model performs when it is allowed to \"choose its battles\" by abstaining from uncertain predictions. In general:\n",
    "- Higher thresholds (more stringent confidence requirements) lead to fewer predictions (lower coverage) but usually higher selective accuracy.\n",
    "- Lower thresholds result in more predictions (higher coverage) but potentially lower selective accuracy.\n",
    "\n",
    "#### Key Trade-Off\n",
    "There is an important trade-off between **coverage** and **accuracy**:\n",
    "- If you want high coverage (making predictions on almost all instances), you might sacrifice accuracy, as the model will include uncertain cases.\n",
    "- If you prioritize accuracy, coverage will decrease because the model abstains more frequently.\n",
    "\n",
    "This trade-off depends on the application and the cost of incorrect predictions versus the cost of abstaining.\n",
    "\n",
    "#### Example in Practice\n",
    "Suppose a medical diagnostic model assigns probabilities to different diseases for patients. The user sets a threshold of 90% confidence. For patients where the model predicts with less than 90% confidence, it abstains, asking for a human doctor's review instead. Over the cases where predictions are made, the accuracy of these predictions is calculated.\n",
    "\n",
    "By using this approach, you can ensure that the model is only relied upon when its predictions are trustworthy, enhancing its utility in real-world scenarios."
   ],
   "metadata": {
    "id": "c9kShRw33nVi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "def accuracy_by_threshold(model, x, labels, threshold):\n",
    "    probs = model(x).numpy()\n",
    "    above_threshold = np.max(probs, axis=1) > threshold\n",
    "    correct = np.argmax(probs, axis=1) == np.squeeze(labels)\n",
    "    return (\n",
    "        threshold,\n",
    "        100\n",
    "        * np.sum(np.logical_and(above_threshold, correct))\n",
    "        / np.sum(above_threshold),\n",
    "        100 * np.sum(above_threshold) / x.shape[0],\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_accuracy_by_threshold(model, x, labels):\n",
    "    values = np.array(\n",
    "        [\n",
    "            accuracy_by_treshold(model, x, labels, threshold)\n",
    "            for threshold in np.arange(0.0, 1, 0.01)\n",
    "        ]\n",
    "    )\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(values[:, 0], values[:, 1], \"g-\")\n",
    "    ax2.plot(values[:, 0], values[:, 2], \"b-\")\n",
    "\n",
    "    ax1.set_xlabel(\"Decision Threshold\")\n",
    "    ax1.set_ylabel(\"Accuracy\", color=\"g\")\n",
    "    ax2.set_ylabel(\"Coverage\", color=\"b\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return np.mean(values[:, 1]), np.mean(values[:, 2])"
   ],
   "metadata": {
    "id": "eifbN45X1Q3g"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# MNIST:  Plot Accuracy / Coverage evolution for different thresholds\n",
    "area_acc, area_coverage = plot_accuracy_by_treshold(cnn_model, x_test, y_test)\n",
    "print(\"Area under Accuracy on MNIST test set: \", str(area_acc))\n",
    "print(\"Area under Coverage on MNIST test set: \", str(area_coverage))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "JUAPWXSB6uJ1",
    "outputId": "660d3ca6-14e3-4924-a038-0ebc744396c0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# MNIST-C:  Plot Accuracy / Coverage evolution for different thresholds\n",
    "area_acc, area_coverage = plot_accuracy_by_treshold(cnn_model, x_c_test, y_c_test)\n",
    "print(\"Area under Accuracy on corrupted MNIST test set: \", str(area_acc))\n",
    "print(\"Area under Coverage on corrupted MNIST test set: \", str(area_coverage))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "Y2n1w-RK3xsy",
    "outputId": "0d2b9478-adbc-4b20-bc5f-f633047a4ca0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Exercise 3: Threshold Selection for Desired Accuracy\n",
    "\n",
    "A user of your machine learning model specifies that their application requires a minimum **accuracy of 99%** for predictions. Your task is to determine the **threshold** for confidence predictions that ensures this accuracy, and compute the resulting **coverage** (the fraction of instances for which the model makes predictions).\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. Use the provided function:\n",
    "   ```python\n",
    "   _, accuracy, coverage = accuracy_by_threshold(cnn_model, input, labels, threshold)\n",
    "   ```\n",
    "   to compute the accuracy and coverage for different thresholds.\n",
    "\n",
    "2. Perform the computations on:\n",
    "   - The **MNIST dataset**.\n",
    "   - The **Corrupted MNIST dataset**.\n",
    "\n",
    "#### Questions:\n",
    "1. What threshold ensures at least 99% accuracy for the MNIST dataset? What is the corresponding coverage?\n",
    "2. What threshold ensures at least 99% accuracy for the Corrupted MNIST dataset? What is the corresponding coverage?\n",
    "\n",
    "This exercise will help you understand how to adjust a probabilistic model to meet user-defined accuracy requirements and analyze the impact of data quality on coverage."
   ],
   "metadata": {
    "id": "u4FCn1GEEVJv"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uK4_Z5B1jw3m"
   },
   "source": [
    "# Ensemble of CNN models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "An **ensemble of neural networks** is a method that combines the predictions of multiple individual models to improve overall performance and robustness. Each neural network in the ensemble independently predicts a probability vector (via the softmax activation function) for a given input. These probability vectors represent the model's confidence for each class.\n",
    "\n",
    "The final ensemble prediction is obtained by **averaging the probability distributions** produced by all individual models. Mathematically, for an ensemble of \\(N\\) models, the final predicted probability for class \\(c\\) is:\n",
    "\n",
    "$$\n",
    "P_{\\text{ensemble}}(c) = \\frac{1}{N} \\sum_{i=1}^N P_{\\text{model}_i}(c)\n",
    "$$\n",
    "\n",
    "where $P_{\\text{model}_i}(c)$ is the probability assigned to class \\(c\\) by the \\(i\\)-th model.\n",
    "\n",
    "**Key Points**:\n",
    "1. **Reduced Variance**: Averaging smooths out the noise from individual models, leading to more stable and reliable predictions.\n",
    "2. **Improved Generalization**: By leveraging the diversity of individual models (e.g., trained with different initializations or subsets of data), the ensemble typically outperforms any single model.\n",
    "3. **Interpretation**: The final averaged probability distribution reflects a consensus among all the models, providing a robust measure of confidence in the predictions."
   ],
   "metadata": {
    "id": "TsaZR7HKCdTL"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qaXyMfH5jw30",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e70e358b-dc28-4679-b63e-1b3e687f368e"
   },
   "source": [
    "# Let us create an esemble of CNNs models.\n",
    "ENSEMBLE_SIZE = 2\n",
    "\n",
    "ensemble_cnns = []\n",
    "\n",
    "for i in range(ENSEMBLE_SIZE):\n",
    "    tf.random.set_seed(i)\n",
    "    cnn_model_i = Sequential(\n",
    "        [\n",
    "            Conv2D(\n",
    "                kernel_size=(5, 5),\n",
    "                filters=8,\n",
    "                activation=\"relu\",\n",
    "                padding=\"VALID\",\n",
    "                input_shape=(28, 28, 1),\n",
    "            ),\n",
    "            MaxPooling2D(pool_size=(6, 6)),\n",
    "            Flatten(),\n",
    "            # Adding SoftMax Layer makes the output a probability distribution\n",
    "            Dense(units=10, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    cnn_model_i.compile(\n",
    "        loss=SparseCategoricalCrossentropy(), optimizer=RMSprop(), metrics=[\"accuracy\"]\n",
    "    )\n",
    "    ensemble_cnns.append(cnn_model_i)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "outputId": "77bd7509-d286-456a-a321-9a53d93456ca",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "E9Om-sWxjw30"
   },
   "source": [
    "# Print the model summary\n",
    "ensemble_cnns[0].summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "outputId": "b596ab39-0992-4ac4-d639-8344a2ae2213",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EN0_qWTtjw31"
   },
   "source": [
    "# Train the model\n",
    "for i in range(ENSEMBLE_SIZE):\n",
    "    ensemble_cnns[i].fit(x_train, y_train, epochs=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "outputId": "34efdc56-240d-4d3c-c0f8-f4b21918dfa1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZQ3XMlUjw31"
   },
   "source": [
    "# Evaluate the model\n",
    "for i in range(ENSEMBLE_SIZE):\n",
    "    print(\"Model \", str(i), \":\")\n",
    "    print(\n",
    "        \"Accuracy on MNIST test set: \",\n",
    "        str(ensemble_cnns[i].evaluate(x_test, y_test, verbose=False)[1]),\n",
    "    )\n",
    "    print(\n",
    "        \"Accuracy on corrupted MNIST test set: \",\n",
    "        str(ensemble_cnns[i].evaluate(x_c_test, y_c_test, verbose=False)[1]),\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWaQ4yyEjw31"
   },
   "source": [
    "## Analyse the model predictions\n",
    "\n",
    "We will now do some deeper analysis by looking at the probabilities the model assigns to each class instead of its single prediction.\n",
    "\n",
    "The function below will be useful to help us analyse the probabilistic model predictions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R0PZGlBHjw31"
   },
   "source": [
    "# Function to make plots of the probabilities that the model estimates for an image\n",
    "\n",
    "\n",
    "def analyse_model_prediction_ensemble(data, true_labels, models, image_num):\n",
    "    ensemble_size = len(models)\n",
    "\n",
    "    image = data[image_num]\n",
    "    true_label = true_labels[image_num, 0]\n",
    "    predicted_probabilities = np.empty(shape=(ensemble_size, 10))\n",
    "    for i in range(ensemble_size):\n",
    "        predicted_probabilities[i] = models[i](image[np.newaxis, :]).numpy()[0]\n",
    "    model_prediction = models[i](image[np.newaxis, :])\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        nrows=1, ncols=2, figsize=(10, 2), gridspec_kw={\"width_ratios\": [2, 4]}\n",
    "    )\n",
    "\n",
    "    # Show the image and the true label\n",
    "    ax1.imshow(image[..., 0], cmap=\"gray\")\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.set_title(\"True label: {}\".format(str(true_label)))\n",
    "\n",
    "    # Show a 95% prediction interval of model predicted probabilities\n",
    "    pct_2p5 = np.array(\n",
    "        [np.percentile(predicted_probabilities[:, i], 2.5) for i in range(10)]\n",
    "    )\n",
    "    pct_97p5 = np.array(\n",
    "        [np.percentile(predicted_probabilities[:, i], 97.5) for i in range(10)]\n",
    "    )\n",
    "    bar = ax2.bar(np.arange(10), pct_97p5, color=\"red\")\n",
    "    bar[int(true_label)].set_color(\"green\")\n",
    "    ax2.bar(\n",
    "        np.arange(10), pct_2p5 - 0.02, color=\"white\", linewidth=1, edgecolor=\"white\"\n",
    "    )\n",
    "    ax2.set_xticks(np.arange(10))\n",
    "    ax2.set_ylim([0, 1])\n",
    "    ax2.set_ylabel(\"Probability\")\n",
    "    ax2.set_title(\"Model estimated probabilities\")\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "outputId": "dd6f766c-dd2e-46f9-d167-6300f526799d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "30aq83ZOjw31"
   },
   "source": [
    "# Prediction examples on MNIST\n",
    "\n",
    "for i in [0, 1577]:\n",
    "    analyse_model_prediction_ensemble(x_test, y_test, ensemble_cnns, i)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "outputId": "5ada6ccc-861d-413a-d4c2-3c1569228077",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "lug6pVt2jw32"
   },
   "source": [
    "# Prediction examples on MNIST-C\n",
    "\n",
    "for i in [0, 3710]:\n",
    "    analyse_model_prediction_ensemble(x_c_test, y_c_test, ensemble_cnns, i)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "outputId": "37095723-f979-4415-e6f9-a01e67614aec",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "lxlRrSY_jw34"
   },
   "source": [
    "# Prediction examples from both datasets\n",
    "\n",
    "for i in [9241]:\n",
    "    analyse_model_prediction_ensemble(x_test, y_test, ensemble_cnns, i)\n",
    "    analyse_model_prediction_ensemble(x_c_test, y_c_test, ensemble_cnns, i)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaMMK14xjw34"
   },
   "source": [
    "It's not surprising what's happening here: the spatters cover up most of the number. You would hope a model indicates that it's unsure here, since there's very little information to go by. This is exactly what's happened."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 5:\n",
    "\n",
    "Repeat Excercise 1 and Compare the output of the ensemble with the output of the single model"
   ],
   "metadata": {
    "id": "l2E70sAYGzTM"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWzm4FjUjw34"
   },
   "source": [
    "## Uncertainty quantification using entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YtuWXUw8jw34"
   },
   "source": [
    "# Functions to plot the distribution of the information entropy across samples,\n",
    "# split into whether the model prediction is correct or incorrect\n",
    "\n",
    "\n",
    "def combine_ensemble(probabilistic_models, x):\n",
    "    probs_ensemble = tf.stack(\n",
    "        [probabilistic_models[i](x) for i in range(len(probabilistic_models))]\n",
    "    )\n",
    "    return tf.reduce_mean(probs_ensemble, axis=0)\n",
    "\n",
    "\n",
    "def get_correct_indices_ensemble(probabilistic_models, x, labels):\n",
    "    logits_valid = tf.stack(\n",
    "        [\n",
    "            tf.math.log(probabilistic_models[i](x))\n",
    "            for i in range(len(probabilistic_models))\n",
    "        ]\n",
    "    )\n",
    "    logs = tf.nn.log_softmax(tf.math.reduce_logsumexp(logits_valid, axis=0))\n",
    "\n",
    "    correct = np.argmax(logs, axis=1) == np.squeeze(labels)\n",
    "    correct_indices = [i for i in range(x.shape[0]) if correct[i]]\n",
    "    incorrect_indices = [i for i in range(x.shape[0]) if not correct[i]]\n",
    "    return correct_indices, incorrect_indices\n",
    "\n",
    "\n",
    "def plot_entropy_distribution_ensemble(probabilistic_models, x, labels):\n",
    "    logits_valid = tf.stack(\n",
    "        [\n",
    "            tf.math.log(probabilistic_models[i](x))\n",
    "            for i in range(len(probabilistic_models))\n",
    "        ]\n",
    "    )\n",
    "    logs = tf.nn.log_softmax(tf.math.reduce_logsumexp(logits_valid, axis=0))\n",
    "\n",
    "    entropy = -tf.reduce_sum(tf.exp(logs) * logs, axis=1).numpy()\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    for i, category in zip(range(2), [\"Correct\", \"Incorrect\"]):\n",
    "        entropy_category = entropy[\n",
    "            get_correct_indices_ensemble(probabilistic_models, x, labels)[i]\n",
    "        ]\n",
    "        mean_entropy = np.mean(entropy_category)\n",
    "        num_samples = entropy_category.shape[0]\n",
    "        title = category + \"ly labelled ({:.1f}% of total)\".format(\n",
    "            num_samples / x.shape[0] * 100\n",
    "        )\n",
    "        axes[i].hist(entropy_category, weights=(1 / num_samples) * np.ones(num_samples))\n",
    "        axes[i].annotate(\n",
    "            \"Mean: {:.3f} bits\".format(mean_entropy), (0.4, 0.9), ha=\"center\"\n",
    "        )\n",
    "        axes[i].set_xlabel(\"Entropy (bits)\")\n",
    "        axes[i].set_ylim([0, 1])\n",
    "        axes[i].set_ylabel(\"Probability\")\n",
    "        axes[i].set_title(title)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "outputId": "0df56651-2209-4c2e-8324-675fff5e661a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "HmvCR150jw34"
   },
   "source": [
    "# Entropy plots for the MNIST dataset\n",
    "\n",
    "print(\"MNIST test set:\")\n",
    "plot_entropy_distribution_ensemble(ensemble_cnns, x_test, y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "outputId": "072725ed-d736-4b3f-bf35-cbb2ca8ab3a1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "Jz-VqQFujw34"
   },
   "source": [
    "# Entropy plots for the MNIST-C dataset\n",
    "\n",
    "print(\"Corrupted MNIST test set:\")\n",
    "plot_entropy_distribution_ensemble(ensemble_cnns, x_c_test, y_c_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 6:\n",
    "\n",
    "Compare the entropy of the ensemble with the entropy of an individual model analyzed in the previous section."
   ],
   "metadata": {
    "id": "7t9twoU5HZ28"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using a Ensemble of Probabilistic Models for Selective Predictions\n"
   ],
   "metadata": {
    "id": "81VnQ9UmAqc1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "def accuracy_by_threshold_ensemble(probabilistic_models, x, labels, threshold):\n",
    "    probs = combine_ensemble(probabilistic_models, x).numpy()\n",
    "    above_threshold = np.max(probs, axis=1) > threshold\n",
    "    correct = np.argmax(probs, axis=1) == np.squeeze(labels)\n",
    "    return (\n",
    "        threshold,\n",
    "        100\n",
    "        * np.sum(np.logical_and(above_threshold, correct))\n",
    "        / np.sum(above_threshold),\n",
    "        100 * np.sum(above_threshold) / x.shape[0],\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_accuracy_by_threshold_ensemble(model, x, labels):\n",
    "    values = np.array(\n",
    "        [\n",
    "            accuracy_by_threshold_ensemble(model, x, labels, threshold)\n",
    "            for threshold in np.arange(0.0, 1, 0.01)\n",
    "        ]\n",
    "    )\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(values[:, 0], values[:, 1], \"g-\")\n",
    "    ax2.plot(values[:, 0], values[:, 2], \"b-\")\n",
    "\n",
    "    ax1.set_xlabel(\"Decision Threshold\")\n",
    "    ax1.set_ylabel(\"Accuracy\", color=\"g\")\n",
    "    ax2.set_ylabel(\"Coverage\", color=\"b\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return np.mean(values[:, 1]), np.mean(values[:, 2])"
   ],
   "metadata": {
    "id": "YZGwN0KkAqdA"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# MNIST:  Plot Accuracy / Coverage evolution for different thresholds\n",
    "area_acc, area_coverage = plot_accuracy_by_threshold_ensemble(\n",
    "    ensemble_cnns, x_test, y_test\n",
    ")\n",
    "print(\"Area under Accuracy on MNIST test set: \", str(area_acc))\n",
    "print(\"Area under Coverage on MNIST test set: \", str(area_coverage))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "outputId": "18d99353-5960-4b2e-f6cc-39ddd8ebba5e",
    "id": "QJSHioxbAqdA"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# MNIST-C:  Plot Accuracy / Coverage evolution for different thresholds\n",
    "area_acc, area_coverage = plot_accuracy_by_threshold_ensemble(\n",
    "    ensemble_cnns, x_c_test, y_c_test\n",
    ")\n",
    "print(\"Area under Accuracy on corrupted MNIST test set: \", str(area_acc))\n",
    "print(\"Area under Coverage on corrupted MNIST test set: \", str(area_coverage))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "outputId": "5d285084-64ad-432d-f0cf-5174645bd000",
    "id": "IoWzWyn8AqdA"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 7:\n",
    "\n",
    "Repeat Excercise 3 using this ensemble of neural networks and compare and discuss the results.\n",
    "\n",
    "\n",
    "Use the function:\n",
    "\n",
    "```python\n",
    "_, accuracy, coverage = accuracy_by_threshold_ensemble(ensemble_cnns, input, labels, threshold):\n",
    "```"
   ],
   "metadata": {
    "id": "ZVp8FOypIc78"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 8: Impact of Ensemble Size on Performance Metrics\n",
    "\n",
    "In this exercise, you will explore how increasing the number of models in an ensemble affects the outcomes of the tasks addressed in Exercises 5, 6, and 7. Previously, you worked with an ensemble consisting of only two models. Your goal is to investigate whether adding more models to the ensemble leads to improvements in the performance metrics evaluated in those exercises. Specifically, you will:\n",
    "\n",
    "1. **Replicate Experiments**: Repeat the experiments from Exercises 5, 6, and 7, using ensembles with varying numbers of models (e.g., 3, 5, 10, etc.).\n",
    "\n",
    "2. **Analyze Changes**: Assess how the performance metrics from Exercises 5, 6, and 7 change as the ensemble size increases.\n",
    "\n",
    "3. **Identify Trends**: Determine whether there is a point of diminishing returns, where adding more models no longer provides significant improvements or leads to overfitting.\n",
    "\n",
    "4. **Discuss Findings**: Summarize your findings, focusing on:\n",
    "   - The relationship between ensemble size and performance.\n",
    "   - Any trade-offs observed, such as increased computational cost or memory usage.\n",
    "   - Implications for practical applications where ensemble size is a factor.\n",
    "\n",
    "This exercise will help you understand how ensemble size influences the effectiveness of combined models and the associated computational trade-offs."
   ],
   "metadata": {
    "id": "vIfCC8p6JwJd"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
