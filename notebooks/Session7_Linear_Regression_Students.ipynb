{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_qJzszxBKsg"
   },
   "source": [
    "# Session 7: Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7q8ba9mBKsh"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Packages\n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment.\n",
    "- [numpy](www.numpy.org) is the fundamental package for working with matrices in Python.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- ``utils.py`` contains helper functions for this assignment. You do not need to modify code in this file.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3GFCseHmBKsh"
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XxzbjgkfBPUk"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    data = np.loadtxt(\n",
    "        \"https://drive.google.com/uc?id=1AkapW4GntdONcHy7God30DETeUPfgax6\",\n",
    "        delimiter=\",\",\n",
    "        skiprows=1,\n",
    "    )\n",
    "\n",
    "    X = data[:, 0] / 1000.0\n",
    "    y = data[:, 4]\n",
    "\n",
    "    return X, y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PWTt-KlhWyIR"
   },
   "source": [
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "def feedback(excercise_id, finished=True, understood=True, comment=\"No comments\"):\n",
    "    base_url = \"https://docs.google.com/forms/d/e/\"\n",
    "    base_url += \"1FAIpQLSdQwo5EswpUpJnx0cHgPdJzuiUsg7KiJMORmC5VZBSGEhjPrQ\"\n",
    "    base_url += \"/formResponse?\"\n",
    "    base_url += \"entry.2058183318=\" + excercise_id\n",
    "    base_url += \"&entry.217828241=\" + (\"Yes\" if finished else \"No\")\n",
    "    base_url += \"&entry.676697552=\" + (\"Yes\" if understood else \"No\")\n",
    "    base_url += \"&entry.379660172=\" + comment\n",
    "\n",
    "    base_url = base_url.replace(\" \", \"%20\")\n",
    "    # urlopen(base_url)\n",
    "    print(\"FEEDBACK SUBMITTED. THANKS!!! :D\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SB-JMDqBKsi"
   },
   "source": [
    "## 2 -  Problem Statement\n",
    "\n",
    "## 2.1 Data Set\n",
    "We will use the motivating example of housing price prediction. The training data set contains many houses with size and its selling price.\n",
    "\n",
    "We would like to build a linear regression model using the size we can then predict the price for other houses - say, a house with 1200 sqft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWS7iEF10i1M"
   },
   "source": [
    "\n",
    "## 3 - Dataset\n",
    "\n",
    "You will start by loading the dataset for this task.\n",
    "- The `load_data()` function shown below loads the data into variables `x_train` and `y_train`\n",
    "  - `x_train` is the size of the house\n",
    "  - `y_train` is the value of the house\n",
    "  - Both `X_train` and `y_train` are numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kaa_g5vdBKsi"
   },
   "source": [
    "# load the dataset\n",
    "x_train, y_train = load_data()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k9nNlJiBKsi"
   },
   "source": [
    "#### View the variables\n",
    "Before starting on any task, it is useful to get more familiar with your dataset.\n",
    "- A good place to start is to just print out each variable and see what it contains.\n",
    "\n",
    "The code below prints the variable `x_train` and the type of the variable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMses7DGBKsi",
    "outputId": "d3631b4d-240d-44dd-9361-7f0c8d57ba9c"
   },
   "source": [
    "# print x_train\n",
    "print(\"Type of x_train:\", type(x_train))\n",
    "print(\"First five elements of x_train are:\\n\", x_train[:5])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zeeQbv5cBKsi",
    "outputId": "f69b8216-26ac-493c-e34f-e817978d0f10"
   },
   "source": [
    "# print y_train\n",
    "print(\"Type of y_train:\", type(y_train))\n",
    "print(\"First five elements of y_train are:\\n\", y_train[:5])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lCmYf3LBKsj"
   },
   "source": [
    "#### Check the dimensions of your variables\n",
    "\n",
    "Another useful way to get familiar with your data is to view its dimensions.\n",
    "\n",
    "Please print the shape of `x_train` and `y_train` and see how many training examples you have in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMLwRWkWBKsj",
    "outputId": "f64812eb-3db0-43af-8af6-f9aa13cee4ac"
   },
   "source": [
    "print(\"The shape of x_train is:\", x_train.shape)\n",
    "print(\"The shape of y_train is: \", y_train.shape)\n",
    "print(\"Number of training examples (m):\", len(x_train))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TFiOtcrBKsj"
   },
   "source": [
    "#### Visualize your data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "TpO9h3FWBKsj",
    "outputId": "d6ded6ea-9cb2-42b6-88f8-8500121901f2"
   },
   "source": [
    "# Create a scatter plot of the data. To change the markers to red \"x\",\n",
    "# we used the 'marker' and 'c' parameters\n",
    "plt.scatter(x_train, y_train, marker=\"x\", c=\"r\")\n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Size vs. Price\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel(\"Price in $1,000\")\n",
    "# Set the x-axis label\n",
    "plt.xlabel(\"Size in 1,000 square feet meters\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "356ZVzAQCrQM"
   },
   "source": [
    "## 4 Model function\n",
    "\n",
    "As described in lecture, the model function for linear regression (which is a function that maps from `x` to `y`) is represented as\n",
    "\n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "\n",
    "The formula above is how you can represent straight lines - different values of $w$ and $b$ give you different straight lines on the plot. <br/> <br/> <br/>\n",
    "\n",
    "Now, let's compute the value of $f_{w,b}(x^{(i)})$ for your two data points. You can explicitly write this out for each data point as -\n",
    "\n",
    "for $x^{(0)}$, `f_wb = w * x[0] + b`\n",
    "\n",
    "for $x^{(1)}$, `f_wb = w * x[1] + b`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nDlVlQaRaph"
   },
   "source": [
    "\n",
    "<a name=\"ex01\"></a>\n",
    "### Exercise 1: Model Predition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kGogRsiPWhi"
   },
   "source": [
    "\n",
    "Complete the function `compute_model_output` to compute the output of a linear regression model given a vector of $x$ values and a two parameters $w$ and $b$.\n",
    "\n",
    "> **Note**: The argument description `(ndarray (m,))` describes a Numpy n-dimensional array of shape (m,). `(scalar)` describes an argument without dimensions, just a magnitude.\n",
    "> **Note**: `np.zero(n)` will return a one-dimensional numpy array with $n$ entries\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2Ul8Ls6QPWhi"
   },
   "source": [
    "def compute_model_output(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the prediction of a linear model\n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples\n",
    "      w,b (scalar)    : model parameters\n",
    "    Returns\n",
    "      y (ndarray (m,)): target values\n",
    "    \"\"\"\n",
    "\n",
    "    m = x.shape[0]\n",
    "    f_wb = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        f_wb[i] = w * x[i] + b\n",
    "\n",
    "    return f_wb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulUsHIhgPWhi"
   },
   "source": [
    "Now let's call the `compute_model_output` function and plot the output.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29qvktYrRxxZ"
   },
   "source": [
    "\n",
    "\n",
    "Let's try to get a better intuition for this through the code blocks below. Let's start with $w = 1$ and $b = 4$.\n",
    "\n",
    "**Note: You can come back to this cell to adjust the model's w and b parameters**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaRvQf3LPWhi",
    "outputId": "21daf302-d6b3-4e43-8bf8-2e9131c7b025"
   },
   "source": [
    "w = 250\n",
    "b = 0\n",
    "print(f\"w: {w}\")\n",
    "print(f\"b: {b}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "JOP1qd7YPWhj",
    "outputId": "363fe379-e62e-4fb2-9c13-6af66f75b072"
   },
   "source": [
    "tmp_f_wb = compute_model_output(x_train, w, b)\n",
    "\n",
    "# Plot our model prediction\n",
    "plt.plot(x_train, tmp_f_wb, c=\"b\", label=\"Our Prediction\")\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(x_train, y_train, marker=\"x\", c=\"r\", label=\"Actual Values\")\n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Size vs. Price\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel(\"Price in $1,000\")\n",
    "# Set the x-axis label\n",
    "plt.xlabel(\"Size in 1,000 square feet meters\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVIjmfJ2PWhj"
   },
   "source": [
    "As you can see, setting $w = 1000$ and $b = 10$ does *not* result in a line that fits our data.\n",
    "\n",
    "Try experimenting with different values of $w$ and $b$. What should the values be for a line that fits our data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWbhyxpDh8q_"
   },
   "source": [
    "\n",
    "---\n",
    "SUBMIT FEEDBACK\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GE3tMDUVh8rM",
    "outputId": "cbd884c3-5ada-43d8-bb42-11cf65c02a3f"
   },
   "source": [
    "feedback(\"session2_Exercise1\", finished=True, understood=True, comment=\"No comments\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-2VcxvVBKsj"
   },
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Compute Cost\n",
    "\n",
    "Gradient descent involves repeated steps to adjust the value of your parameter $(w,b)$ to gradually get a smaller and smaller cost $J(w,b)$.\n",
    "- At each step of gradient descent, it will be helpful for you to monitor your progress by computing the cost $J(w,b)$ as $(w,b)$ gets updated.\n",
    "- In this section, you will implement a function to calculate $J(w,b)$ so that you can check the progress of your gradient descent implementation.\n",
    "\n",
    "#### Cost function\n",
    "As you may recall from the lecture, for one variable, the cost function for linear regression $J(w,b)$ is defined as\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "- You can think of $f_{w,b}(x^{(i)})$ as the model's prediction of your restaurant's profit, as opposed to $y^{(i)}$, which is the actual profit that is recorded in the data.\n",
    "- $m$ is the number of training examples in the dataset\n",
    "\n",
    "#### Model prediction\n",
    "\n",
    "- For linear regression with one variable, the prediction of the model $f_{w,b}$ for an example $x^{(i)}$ is representented as:\n",
    "\n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b$$\n",
    "\n",
    "This is the equation for a line, with an intercept $b$ and a slope $w$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSuype1SBKsj"
   },
   "source": [
    "### Exercise 2: Cost Function\n",
    "\n",
    "Complete the code of the `compute_cost` methdod below to:\n",
    "\n",
    "* Iterate over the training examples, and for each example, compute:\n",
    "    * The prediction of the model for that example\n",
    "    $$\n",
    "    f_{wb}(x^{(i)}) =  wx^{(i)} + b\n",
    "    $$\n",
    "\n",
    "    * The cost for that example  $$cost^{(i)} =  (f_{wb}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "\n",
    "* Return the total cost over all examples\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} cost^{(i)}$$\n",
    "  * Here, $m$ is the number of training examples and $\\sum$ is the summation operator\n",
    "\n",
    "If you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FPsROdkQBKsk"
   },
   "source": [
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): Shape (m,) Input to the model (Population of cities)\n",
    "        y (ndarray): Shape (m,) Label (Actual profits for the cities)\n",
    "        w, b (scalar): Parameters of the model\n",
    "\n",
    "    Returns\n",
    "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
    "               to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = x.shape[0]\n",
    "\n",
    "    cost_sum = 0\n",
    "    for i in range(m):\n",
    "        # Compute the prediction for the price of the i-th house\n",
    "        fwb_i = w * x[i] + b\n",
    "        cost_i = (fwb_i - y[i]) ** 2\n",
    "        cost_sum = cost_sum + cost_i\n",
    "\n",
    "    total_cost = cost_sum / (2 * m)\n",
    "\n",
    "    return total_cost"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0WmrRqABKsk"
   },
   "source": [
    "You can check if your implementation was correct by running the following test code:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shwUqfuYBKsk",
    "outputId": "6d0f612e-a9af-4e9d-a973-5884875a067f"
   },
   "source": [
    "# Compute cost with some initial values for paramaters w, b\n",
    "initial_w = 1000\n",
    "initial_b = 10\n",
    "\n",
    "cost = compute_cost(x_train, y_train, initial_w, initial_b)\n",
    "print(type(cost))\n",
    "print(f\"Cost at initial w (zeros): {cost:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anPRX3O8Fonq"
   },
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Cost at w=1000, b=10:<b> 620793.518 </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JBOIOf2xQeq"
   },
   "source": [
    "\n",
    "---\n",
    "SUBMIT FEEDBACK\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzdNBMomxYIN",
    "outputId": "b00bd838-4ba1-4604-d0dc-c26a25eb2fcc"
   },
   "source": [
    "feedback(\"session2_Exercise2\", finished=True, understood=True, comment=\"No comments\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYIqH7P-KJF5",
    "outputId": "1ab7f441-0b47-4067-c7d7-404bd6a2206f"
   },
   "source": [
    "math.sqrt(2 * cost)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1jNBob6MJDc"
   },
   "source": [
    "### Plotting the cost function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "UpS_eggcMhiE",
    "outputId": "3edbed01-4151-4de2-d6b6-dcf3a6c4e0ee"
   },
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "# Axes3D import has side effects, it enables using projection='3d' in add_subplot\n",
    "def fun(x, y):\n",
    "    return x**2 + y\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "w = b = np.arange(0, 300.0, 1)\n",
    "W, B = np.meshgrid(w, b)\n",
    "js = np.array(compute_cost(x_train, y_train, np.ravel(W), np.ravel(B)))\n",
    "J = js.reshape(W.shape)\n",
    "\n",
    "ax.plot_surface(W, B, J)\n",
    "\n",
    "ax.set_xlabel(\"w\")\n",
    "ax.set_ylabel(\"b\")\n",
    "ax.set_zlabel(\"J\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9HqS5nYKtA9"
   },
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Gradient at test w=0, b=0 <b></td>\n",
    "    <td> 1643.6115982424244 1065.2176565656566</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ru8vJpfwBKsk"
   },
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6 - Gradient descent\n",
    "\n",
    "In this section, you will implement the gradient for parameters $w, b$ for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6wRB6b2BKsk"
   },
   "source": [
    "As described in the lecture videos, the gradient descent algorithm is:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\phantom {0000} b := b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\newline       \\; & \\phantom {0000} w := w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{1}  \\; &\n",
    "\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "where, parameters $w, b$ are both updated simultaniously and where\n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)} \\tag{3}\n",
    "$$\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "\n",
    "*  $f_{w,b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$, is the target value\n",
    "\n",
    "\n",
    "You will implement a function called `compute_gradient` which calculates $\\frac{\\partial J(w)}{\\partial w}$, $\\frac{\\partial J(w)}{\\partial b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOjzW8lXBKsk"
   },
   "source": [
    "<a name=\"ex03\"></a>\n",
    "### Exercise 3: Compute Gradient\n",
    "\n",
    "Please complete the `compute_gradient` function to:\n",
    "\n",
    "* Iterate over the training examples, and for each example, compute:\n",
    "    * The prediction of the model for that example\n",
    "    $$\n",
    "    f_{wb}(x^{(i)}) =  wx^{(i)} + b\n",
    "    $$\n",
    "\n",
    "    * The gradient for the parameters $w, b$ from that example\n",
    "        $$\n",
    "        \\frac{\\partial J(w,b)}{\\partial b}^{(i)}  =  (f_{w,b}(x^{(i)}) - y^{(i)})\n",
    "        $$\n",
    "        $$\n",
    "        \\frac{\\partial J(w,b)}{\\partial w}^{(i)}  =  (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)}\n",
    "        $$\n",
    "\n",
    "\n",
    "* Return the total gradient update from all the examples\n",
    "    $$\n",
    "    \\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} \\frac{\\partial J(w,b)}{\\partial b}^{(i)}\n",
    "    $$\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} \\frac{\\partial J(w,b)}{\\partial w}^{(i)}\n",
    "    $$\n",
    "  * Here, $m$ is the number of training examples and $\\sum$ is the summation operator\n",
    "\n",
    "If you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "peJThQp-FPNW"
   },
   "source": [
    "def compute_gradient(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples\n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters\n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of training examples\n",
    "    m = x.shape[0]\n",
    "\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = x[i] * w + b\n",
    "\n",
    "        dj_dw += (f_wb_i - y[i]) * x[i]\n",
    "        dj_db += f_wb_i - y[i]\n",
    "\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_dw, dj_db"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SzmbidjBKsl"
   },
   "source": [
    "Run the cells below to check your implementation of the `compute_gradient` function with two different initializations of the parameters $w$,$b$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IaubX95yK0dg",
    "outputId": "71742bd2-088c-4fac-8891-53d470863f14"
   },
   "source": [
    "# Compute and display gradient with w initialized to zeroes\n",
    "initial_w = 0\n",
    "initial_b = 0\n",
    "\n",
    "tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, initial_w, initial_b)\n",
    "print(\"Gradient at initial w, b (zeros):\", tmp_dj_dw, tmp_dj_db)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Q2kfaQ0K0dh"
   },
   "source": [
    "Now let's run the gradient descent algorithm implemented above on our dataset.\n",
    "\n",
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Gradient at  w=0, b=0 <b></td>\n",
    "    <td> -551.7836239797981 -363.1560808080808</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSCAeAA4K0dh",
    "outputId": "91e68647-304a-48a5-fef0-4664a60cedcc"
   },
   "source": [
    "# Compute and display cost and gradient with non-zero w\n",
    "test_w = 1000\n",
    "test_b = 10\n",
    "tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, test_w, test_b)\n",
    "\n",
    "print(\"Gradient at test w, b:\", tmp_dj_dw, tmp_dj_db)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNu4GKZLK0dh"
   },
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Gradient at test w=1000, b=10 <b></td>\n",
    "    <td> 1643.6115982424244 1065.2176565656566</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxlWvozY5fGm"
   },
   "source": [
    "\n",
    "---\n",
    "SUBMIT FEEDBACK\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBnhk3kD5fGm",
    "outputId": "105b12a7-9137-418e-9c91-2801fa2802c1"
   },
   "source": [
    "feedback(\"session2_Exercise3\", finished=True, understood=True, comment=\"No comments\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrMsfWsjBKsl"
   },
   "source": [
    "<a name=\"ex04\"></a>\n",
    "### Exercise 4:  Updating Equation Gradient Descent\n",
    "\n",
    "\n",
    "You will now find the optimal parameters of a linear regression model by using batch gradient descent. Recall batch refers to running all the examples in one iteration.\n",
    "\n",
    "- **TASK**: Implement the updating equation of gradient descent\n",
    "\n",
    "$$\\begin{align*}&\n",
    "\\phantom {0000} b := b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\newline       \\; & \\phantom {0000} w := w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{1}  \\; &\n",
    "\\newline & \\end{align*}$$\n",
    "\n",
    "\n",
    "\n",
    "- A good way to verify that gradient descent is working correctly is to look\n",
    "at the value of $J(w,b)$ and check that it is decreasing with each step.\n",
    "\n",
    "- Assuming you have implemented the gradient and computed the cost correctly and you have an appropriate value for the learning rate alpha, $J(w,b)$ should never increase and should converge to a steady value by the end of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CyrsffwsBKsl"
   },
   "source": [
    "def gradient_descent(\n",
    "    x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
    "    num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    Args:\n",
    "      x :    (ndarray): Shape (m,)\n",
    "      y :    (ndarray): Shape (m,)\n",
    "      w_in, b_in : (scalar) Initial values of parameters of the model\n",
    "      cost_function: function to compute cost\n",
    "      gradient_function: function to compute the gradient\n",
    "      alpha : (float) Learning rate\n",
    "      num_iters : (int) number of iterations to run gradient descent\n",
    "    Returns\n",
    "      w : (ndarray): Shape (1,) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "\n",
    "    # number of training examples\n",
    "    m = len(x)\n",
    "\n",
    "    # An array to store cost J and w's at each iteration â€” primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = [w_in]\n",
    "    b_history = [b_in]\n",
    "    w = copy.deepcopy(w_in)  # avoid modifying global w within function\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b)\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000:  # prevent resource exhaustion\n",
    "            cost = cost_function(x, y, w, b)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            w_history.append(w)\n",
    "            b_history.append(b)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "\n",
    "    return (\n",
    "        w,\n",
    "        b,\n",
    "        J_history,\n",
    "        w_history,\n",
    "        b_history,\n",
    "    )  # return w and J,w history for graphing"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFOmY7UwBKsl"
   },
   "source": [
    "Now let's run the gradient descent algorithm above to learn the parameters for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "id": "4Ftrg0G8BKsl",
    "outputId": "c9a590a6-1e58-4367-e10d-c003a390bb72"
   },
   "source": [
    "# initialize fitting parameters. Recall that the shape of w is (n,)\n",
    "initial_w = 0.0\n",
    "initial_b = 0.0\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.01\n",
    "\n",
    "w_optimal, b_optimal, J_history, w_history, b_history = gradient_descent(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    initial_w,\n",
    "    initial_b,\n",
    "    compute_cost,\n",
    "    compute_gradient,\n",
    "    alpha,\n",
    "    iterations,\n",
    ")\n",
    "\n",
    "print(\"w,b found by gradient descent:\", w_optimal, b_optimal)\n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Gradient Descent Evolution\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.ylim(0, 10000)\n",
    "# Set the x-axis label\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "\n",
    "# plt.ylim(4,10)\n",
    "\n",
    "# Plot the linear fit\n",
    "plt.plot(np.arange(0, iterations, 1), J_history, c=\"b\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AdFGDJmBKsl"
   },
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b> w, b found by gradient descent after 10000 iterations with alpha = 0.01<b></td>\n",
    "    <td> 216.3873236706632 56.2526725869892</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp-U44mu5hoT"
   },
   "source": [
    "\n",
    "---\n",
    "SUBMIT FEEDBACK\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SSf5_Ock5hoU"
   },
   "source": [
    "feedback(\"session2_Exercise4\", finished=True, understood=True, comment=\"No comments\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI1f-FtKQVHB"
   },
   "source": [
    "### Inspecting behaviour of Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MjULNmMMQa12",
    "outputId": "5262060e-0501-467f-b883-4ee17abad431"
   },
   "source": [
    "for w_t, b_t in zip(w_history, b_history):\n",
    "    tmp_f_wb = compute_model_output(x_train, w_t, b_t)\n",
    "\n",
    "    # Plot our model prediction\n",
    "    plt.plot(x_train, tmp_f_wb, c=\"b\", label=\"Our Prediction\")\n",
    "\n",
    "    # Plot the data points\n",
    "    plt.scatter(x_train, y_train, marker=\"x\", c=\"r\", label=\"Actual Values\")\n",
    "\n",
    "    # Set the title\n",
    "    plt.title(\"Size vs. Price\")\n",
    "    # Set the y-axis label\n",
    "    plt.ylabel(\"Price in $1,000\")\n",
    "    # Set the x-axis label\n",
    "    plt.xlabel(\"Size in 1,000 square feet\")\n",
    "    plt.legend()\n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQCBAW7nSOEc"
   },
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vV4iOTULBKsl"
   },
   "source": [
    "We will now use the final parameters from gradient descent to plot the linear fit.\n",
    "\n",
    "Recall that we can get the prediction for a single example $f(x^{(i)})= wx^{(i)}+b$.\n",
    "\n",
    "To calculate the predictions on the entire dataset, we can loop through all the training examples and calculate the prediction for each example. This is shown in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YeXlmniXBKsl"
   },
   "source": [
    "predicted = compute_model_output(x_train, w_optimal, b_optimal)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "basC9qPcBKsl"
   },
   "source": [
    "We will now plot the predicted values to see the linear fit."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "bMDNxiuYBKsm",
    "outputId": "4050d64f-fd32-42ca-f756-257bcd3741de"
   },
   "source": [
    "# Plot the linear fit\n",
    "plt.plot(x_train, predicted, c=\"b\", label=\"Our Prediction\")\n",
    "\n",
    "# Create a scatter plot of the data.\n",
    "plt.scatter(x_train, y_train, marker=\"x\", c=\"r\", label=\"Actual Values\")\n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Size vs. Price\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel(\"Price in $1,000\")\n",
    "# Set the x-axis label\n",
    "plt.xlabel(\"Size in 1,000 square feet\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Py5wDxVZd5IS"
   },
   "source": [
    "Your final values of $w,b$ can also be used to make predictions on prices. Let's predict what the price would be for houses with sizes of 1000 and 2000 square feet.\n",
    "\n",
    "- The model takes in the size of the house in 1,000 square feet as input.\n",
    "- Therefore, a size of 1000 square feet can be translated into an input to the model as `1`.\n",
    "- Similarly, a size of 2000 square feet can be translated into an input to the model as `2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thbn_sVOd5IT",
    "outputId": "8c5f7189-4952-480c-b6e6-2d78d936ac43"
   },
   "source": [
    "predict1 = 1 * w_optimal + b_optimal\n",
    "print(\"For area = 1,000 we predict a price of $%.0f\" % (predict1 * 1000))\n",
    "\n",
    "predict2 = 2 * w_optimal + b_optimal\n",
    "print(\"For area = 2,000 we predict a price of $%.0f\" % (predict2 * 1000))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5POZF7rd5IU"
   },
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b> For area = 1,000, we predict a selling price of<b></td>\n",
    "    <td> $272,640 </td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> <b> For area = 2,000, we predict a selling price of<b></td>\n",
    "    <td> $489,027</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
